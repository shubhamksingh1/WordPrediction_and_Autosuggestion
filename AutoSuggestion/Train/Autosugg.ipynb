{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autosugg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUyY4jy5k3iRRM1QJuquWq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamksingh1/WordPrediction_and_Autosuggestion/blob/main/AutoSuggestion/Train/Autosugg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvUy95z3lymC"
      },
      "source": [
        "### Importing Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqrWHQTxmJYl",
        "outputId": "6daa0144-447b-4673-de18-f279c4579956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwwHsHdwqCR-"
      },
      "source": [
        "### Preparaing Corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKWDTUOsmKCh"
      },
      "source": [
        "file = open('/content/drive/My Drive/WordPrediction/autosugg.txt','r').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rEafyNXnO5Y",
        "outputId": "dd644925-358d-4b5d-e7d7-72c58b8b1125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi bot\\nHey bot\\nGood morning\\nhi again\\nhi folks\\nhi Mister\\nhi pal!\\nhi there\\nhello everybody\\nhello is anybody there\\nhello robot\\nhi hi\\nhey hey\\nhello there\\nhey bot!\\nhello friend\\ngood morning\\nhello sweet boy\\nhey there\\nhello sweatheart\\nayyyy whaddup\\nhey bot\\nwhats up\\nWell hello there ;)\\nI said, helllllloooooO!!!!\\nHeya\\nWhats up my bot\\nhiii\\nheyho\\nhey, let's talk\\nhey let's talk\\njojojo\\nhey dude\\nhello it is me again\\nwhat up\\nhi there\\nhi\\njop\\nhi friend\\nhi there it's me\\ngood evening\\ngood morning\\ngood afternoon\\n perfect\\n very good\\n great\\n amazing\\n wonderful\\n I am feeling very good\\n I am great\\n I'm good\\n sad\\n very sad\\n unhappy\\n bad\\n very bad\\n awful\\n terrible\\n not very good\\n extremely sad\\n so sad\\n are you a bot?\\n are you a human?\\n am I talking to a bot?\\n am I talking to a human?\\nThanks\\nThank you\\nThank you so much\\nThanks bot\\nThanks for that\\ncheers\\ncheers bro\\nok thanks!\\nperfect thank you\\nthanks a bunch for everything\\nthanks for the help\\nthanks a lot\\namazing, thanks\\ncool, thanks\\ncool thank you\\nthanks\\nthanks!\\nCool. Thanks\\nthanks\\nthanks this is great news\\nthank you\\ngreat thanks\\nThanks!\\ncool thanks\\nThanks a ton\\nthanks a bunch\\nThank you very much\\nHow were you built?\\nHow were you made?\\nTell me how you were made?\\nLet me know how you were made exactly\\nI'd like to know how you were created\\nCan you give me an idea as to how you were created?\\nI want to know how you were formed\\nWhat was the process for making you?\\nCan you explain how you were created?\\nSpecify how you were created?\\nI'd like to know how you were put together?\\nCan you say how you were constructed?\\nHow were you made?\\nin what way were you made?\\nHow were you devised?\\nHow exactly were you made?\\nHow exactly were you devised?\\nso how were you made?\\nHow did you come to be?\\nHow were you constructed?\\nHow were you formed?\\nHow did they make you?\\nHow did they build you?\\nHow did they create you?\\nHow were you made?\\nDo you know how you were made?\\nDo you know how you were built?\\nWhat process was used to create you?\\nWhat process was used to build you?\\nHow did you come into being?\\nHow were you conceived?\\nIn which manner were you devised?\\nHow were you constructed?\\nIn what way were you shaped?\\nBy what means were you made?\\nYou originated through what means?\\nHow were you set up?\\nBy what method were you fashioned?\\ngood\\ngoodnight\\ngood bye\\ngood night\\nsee ya\\nbye bye\\ngotta go\\nfarewell\\ncatch you later\\nbye for now\\nbye\\nbye was nice talking to you\\nbye udo\\nbye bye bot\\nk byyye\\ntalk to you later\\nciao\\nBye bye\\nthen bye\\nbye!\\ntoodleoo\\nb bye!\\nb bye\\ncu later\\ncu\\nnice to meet you\\nbye!! nice to meet you\\nsee u\\nbye was nice to meet you\\nwhats your age?\\nhow old?\\nwhat's your age?\\nwhen were you born?\\nwhat is your birthday?\\nwhen is your birthday?\\nhow many years old are you?\\ndo you know how old you are?\\nwhat is your exact age?\\nwhat year were you born?\\ncan you tell me your age?\\nWhat is your age?\\nWhat is your birthdate?\\nHow old will you be on your next birthday?\\nWhen do you celebrate your day of birth?\\nHow old were you on your last birthday?\\nHow many candles were on your last birthday cake?\\nDo you have friends the same age as you, if so, how old are they?\\nTell me your day, month and year of birth.\\nWhats your age?\\nand you are how many years old?\\nwhats your birth year?\\nwhat age were you when you celebrated your last birthday?\\nhow old were you when you celebrated your last birthday?\\ntell me your age?\\nhow about your age\\nhow old were you on your last birthday?\\nwhat age are you\\nhow old will you be this year?\\nWhat is your age?\\nWhat year were you born?\\nHow long have you been around?\\nHow long have you been alive?\\nWhat was your age on your last birthday?\\nWhat will be your age on your next birthday?\\nHow many years have you lived?\\nHow many years have you been alive?\\nHow long have you occupied the earth?\\nwhere do you live\\nwhere are you from?\\nSo where are you from\\nWhere are you?\\nWhere were you born?\\nWhat city were you born in?\\nWhere did you grow up?\\nWhat state were you born in?\\nWhat country were you born in?\\nWhere do you come from?\\nWhere were you at before you were here?\\nWhere from?\\nDo you know where you come from?\\nwhere's your home town?\\nwhat country are you from?\\nare you from around here?\\nwhere did you grow up?\\nwhere are your parents from?\\nwhere did you spend your youth?\\nwhere did you come from?\\nWhere were you born?\\nWhere do you consider home?\\nWhat city do you claim to for your birth?\\nWhat citizenship do you lay claim to?\\nWhat is your heritage?\\nWhat is your birthplace?\\nWhere did you grow up?\\nYou were conceived in what location?\\nWhat is your root?\\nWhat is your origin?\\nFrom where did you come?\\nWhere did you come from?\\nWhere did you originate?\\nWhere are your roots?\\nWhere are your origins?\\nWhat area are you from?\\nWhat location are you from?\\nWhat's the name of the place you came from?\\nin which city do you live\\nyour residence\\n your house\\n your home\\n your hometown?\\n what is your hometown\\n is it your hometown\\n where is your hometown\\n tell me about your city\\n what is your city\\n what is your residence\\n what is your town\\n what's your city\\n what's your home\\n where is your home\\n where is your residence\\n where's your home\\n where's your hometown\\n where's your house\\n where you live\\n your city\\n your town\\n What is your hometown?\\nAhoy matey how are you?\\nare you alright\\nare you having a good day\\nAre you ok?\\nare you okay\\nDo you feel good?\\nhow are things going\\nhow are things with you?\\nHow are things?\\nhow are you\\nhow are you doing\\nhow are you doing this morning\\nhow are you feeling\\nhow are you today\\nHow are you?\\nhow do you do?\\nhow have you been\\nhow is it going\\nhow is your day\\nhow is your day going\\nhow is your evening\\nHow was your day?\\nHow's it going\\nHow's it hanging?\\nhow's life\\nhow's life been treating you?\\nHow's life treating you friend?\\nhow's your day going\\nHow've you been?\\nI'm fine and you\\nis everything all right\\nIs everything ok?\\nis everything okay\\nwhat about your day\\nWhat's going on?\\nwhat's good\\nWhat's new?\\nWhat's up man\\nWhat's up\\nhow are you?\\nDo you have a great day?\\nhow are you doing today my sweet friend\\nhow are you doing?\\nhow are u\\nHow are you?\\nok then you cant help me\\nthat was shit, you're not helping\\nyou can't help me\\nyou can't help me with what i need\\ni guess you can't help me then\\nok i guess you can't help me\\nthat's not what i want\\nok, but that doesnt help me\\nthis conversation is not really helpful\\nthis conversation is not really helpful\\nI think you cant help me\\nhm i don't think you can do what i want\\nnothing else?\\nand that's it?\\nand you call yourself bot ?\\ncan you share your boss with me?\\ni want to get to know your owner\\ni want to know the company which designed you\\ni want to know the company which generated you\\ni want to know the company which invented you\\ni want to know who invented you\\nMay I ask who invented you?\\nplease tell me the company who created you\\nplease tell me who created you\\ntell me more about your creators\\ntell me more about your founders\\nwhich company brought you into existence?\\nwhich company created you?\\nwhich company invented you?\\nWho's your boss?\\nWho's your builder?\\nWho built you?\\nWho created you?\\nWho is your creator?\\nWho designed you?\\nwho do you work for?\\nWho fabricated you?\\nWho generated you?\\nwho had the idea to build you?\\nwho had the idea to generate you?\\nwho had the idea to make you?\\nwho is behind all this?\\nWho is the legal owner of you?\\nwho is your boss tell me\\nwho is your boss?\\nwho is your creator?\\nWho is your programmer?\\nWho made you?\\nWho modeled you?\\nWho owns you?\\nWho produced you?\\nWho thought to make you?\\nWho went through the trouble of setting you up?\\nand who built u?\\nwho built you?\\nwho build yoi\\nwho made you?\\nwhere were you made?\\nwhy were you made?\\nhow were you made?\\nwho created you?\\nWhat can you do?\\nWhat should I ask you?\\nhow can you help me\\nI need help\\nhelp me\\nI'm stuck\\ni don't know what to do\\nI think you can help me\\ni guess you can help me then\\ni guess you can help\\nWill you help me\\ncan you help me\\nPlease help me\\nIf you don't mind, I really need your help\\nCould you please help me\\nDo you think you can help me?\\nWill you be able to help me?\\nCan you please help me\\nWhat is LIA?\\nWhat do you mean by LIA?\\nI don't know who the LIA is\\nCan you tell me what the LIA is\\nLIA?\\nWhat's LIA?\\nCan you explain me LIA\\nhow to use LIA ?\\nHow do you work ?\\nCan you tell me how you work ?\\nHow does LIA work ? \\nWhat are you ?\\nHow to use you ?\\nwhat is lia\\nwhat is lia ? \\ncan you explain me how you work\\ntell me how you work \\nhow do you work lia\\nwhat are you lia ?\\nwhat do you do\\nwhat are you \\nhow can you help me \\nexplain your working lia\\nexplain your working \\nhow can you help me ?\\nlia how can you help me ?\\nlia how can you help me\\nWhat are course activity?\\ncourse activity?\\nWhat do you mean by course activity?\\nCan you tell me what course activity is ?\\nWhat's course activity?\\nI am unaware of course activity\\nI am not aware of course activity\\nCan you explain me course activity ?\\nwhat are the course activity\\ncourse activity means\\ncourse activity\\nlia what is course activity ?\\nlia what are course activity\\ncan you tell me about course activity ? \\ncourse activity meaning \\ncourse activity means ?\\nlia what do you mean by course activity ?\\nlia what do you mean by course activity\\nlia what do you mean by activity in the course ?\\nlia what does activity in the course mean?\\ntell me what you know about course activity\\nlia tell me what you know about course activity\\ni don't know what i want\\nshow me what's possible\\nso what can you help me with?\\nwhat are you good at?\\nwhat can I do here\\nwhat can I do with this bot\\nwhat is this bot for\\ncool! can I do something else here?\\nhow can you help me\\nwhat else can i do\\nGreat, is there anything else you can do, bot?\\nwhat can you do?\\nhow can you help me?\\ni asked you if you can do anything else\\nwhat are the options?\\nBot!!Please do something for me.\\nis anything else possible for you?\\nanything else?\\nwhat can i do now\\nCan you explain me in one sentence what you are doing?\\nwho is this\\nwho am i talking to\\nwhat's your name\\nwho am i speaking with\\nwho r u\\nwhat's your name bot\\nwhats ur name\\nwhat are you called?\\nwho are you and what do you want from me\\nwho are you?\\nwho is it?\\nwho are u?\\nare you a bot?\\nwho is this?\\nhi! what's your name?\\nam i talking to bot?\\ngive me your introduction.\\nintroduce yourself.\\ngive me your intro.\\nknock knock!!who is it?\\ntell me about yourself\\nyou are annoying me so much\\nyou're incredibly annoying\\nI find you annoying\\nyou are annoying\\nyou're so annoying\\nhow annoying you are\\nyou annoy me\\nyou are annoying me\\nyou are irritating\\nyou are such annoying\\nyou're too annoying\\nyou are very annoying\\nWhy are you annoying me so much\\nHow annoying!\\nIt's annoying\\nThat's annoying\\nIt's just annoying\\nThat's so annoying\\nYou really annoy me\\nYou're starting to annoy me\\nI want you to answer me\\nanswer\\nanswer my question\\nanswer me\\ngive me an answer\\nanswer the question\\ncan you answer my question\\ntell me the answer\\nanswer it\\ngive me the answer\\nI have a question\\nI want you to answer my question\\njust answer the question\\ncan you answer me\\nanswers\\ncan you answer a question for me\\ncan you answer\\nanswering questions\\nI want the answer now\\njust answer my question\\n     You are bad\\n     You are not good\\n     you are very bad\\n     You're bad\\n     You're not good\\n     you're very bad\\n    study\\n    you should study better\\n    you must learn\\n    be clever\\n    be more clever\\n    be smarter\\n    be smart\\n    get qualified\\n    you are not as smart as i thought\\n    you have to learn a lot\\n    you should be trained more\\n    be more smart\\n    you need to learn more\\n    be useful\\n    think out of the box\\n    smarty pants\\n    you need to improve\\n    you should learn\\n    you have to be more smart\\n    u have to use your brains\\n you're cute\\nyou're attractive\\nyou are beautiful\\nyou're looking good today\\nyou are so beautiful\\nyou look amazing\\nyou look so good\\nyou're so gorgeous\\nyou are too beautiful\\nyou look great\\nyou look so well\\nI like the way you look now\\nI think you're beautiful\\nwhy are you so beautiful\\nyou are so beautiful to me\\nyou are cute\\nyou are gorgeous\\nyou are handsome\\nyou are looking awesome\\nyou look amazing today\\nyou are looking beautiful today\\nyou are looking great\\nyou are looking pretty\\nyou are looking so beautiful\\nyou are looking so good\\nyou are pretty\\nyou are really beautiful\\nyou are really cute\\nyou are really pretty\\nyou are so attractive\\nyou are so beautiful today\\nyou are so cute\\nyou are so gorgeous\\nyou are so handsome\\nyou are so pretty\\nyou are very attractive\\nyou are very beautiful\\nyou are very cute\\nyou are very pretty\\nyou look awesome\\nyou look cool\\nyou look fantastic\\nyou look gorgeous\\nyou look great today\\nyou look perfect\\nyou look pretty good\\nyou look so beautiful\\nyou look so beautiful today\\nyou look very pretty\\nyou look wonderful\\nI like the way you look\\nyou look wonderful today\\nyou are cutie\\nyou're looking good\\nyou're pretty\\nhow are you so pretty?\\n What is your birth date?\\n When is your birth date?\\n What's your birth date?\\n When's your birth date?\\n Who is your boss?\\n Who is the boss?\\n Who is your owner?\\n Who owns you?\\n Who's your boss?\\n Who's the boss?\\n Who's your owner?\\n Are you busy?\\n busy?\\n Are you free?\\nTell me a joke\\njoke\\ntell me a joke\\n do you want to eat\\n are you hungry\\n would you like to eat something\\n you are hungry\\n you're so hungry\\n you're very hungry\\n you might be hungry\\n you're really hungry\\n can you eat?\\n how will eat when you are hungry?\\n can you be hungry?\\n how do you eat?\\n Do you feel hungry?\\n are you dying of hunger?\\ni am hungry\\n you are boring\\n you're boring\\n how boring you are\\n you're really boring\\n you're incredibly boring\\n You're boring me\\n you are very boring\\n i find you very boring man\\n you're really boring me\\n i'm bored of you\\n you are not interesting\\n i find you boring\\n you're boring everyone\\n I'm extremely bored because of you\\n Im bored of you\\n I get really bored of you man\\n It's boring\\n You look bored\\n Your answers are terribly boring\\n I was bored with your speech\\ncancel\\nCancel\\ncancel it\\ncancel everything\\ncancel all\\ncancel request\\ncancelled\\nno cancel cancel\\nno just cancel\\ncancel my request\\ncan you cancel that\\ncancel all that\\ncancel this request\\nno cancel this\\nno cancel everything\\ni said cancel\\njust cancel it\\nnothing cancel\\ni want to cancel\\nno just cancel it\\ni said cancel it\\ncancel the whole thing\\ncan you cancel it\\nso cancel\\nnow cancel\\ncancel now\\nsorry cancel\\ncancel that one\\ncancel it cancel it\\ncancel that cancel that\\ncancel all this\\ni want to cancel it\\ni would like to cancel\\nI said cancel cancel\\nbut can you cancel it\\nskip skip skip\\nstop\\nend\\nstop now\\ni want to end the conversation right now\\nstop now!!\\nstop it\\nabort\\nannul\\ndismiss\\ndismissed\\ndisregard\\ndisregard that\\nskip\\nskip it\\nforget that\\nforget\\nforget about it\\ndon't do that\\njust forget it\\ndiscard\\nforget this\\njust forget about it\\nforget about that\\ndo nothing\\njust stop it\\nno stop\\njust forget\\ni said forget it\\nstop go back\\ntell me more\\ncould you please explain that\\ncould you tell me more\\ngive me more information please\\ncould you please give me some more information\\ni want more information\\ncan you explain more information\\nneed more information\\nneed more information about this\\ni require more information\\nyes\\nof course\\nsure\\nyeah\\nok\\ncool\\ngo for it\\nyep\\nyep, will do thank you\\nyep, will do thank you\\nI'm sure I will!\\noh awesome!\\nYes\\naccept\\ndo you want to marry me?\\nudo, I want to marry you\\nis barbara still married to you\\ni want to marry you\\nmarry me\\ncan you marry me\\nI am singe and want to mingle with you\\nwill you marry me\\ni want to marry with you\\nare you single\\nI want pizza\\nplease help with my ice cream it's dripping\\nsomeone call the police i think the bot died\\nyou're a loser lmao\\nyou're mad guy\\nyou're crazy\\ni don't care!!!!\\ni do not care how are you\\nhang on let me find it\\nI changed my mind\\nshit bot\\ngive me food\\ni want food\\nI like you\\nyou are a stupid bot\\ncan we keep chatting?\\ntalk to me\\nlol\\noh my god, not again!\\ncan you tell me how to build a bot?\\ncan you learn from our conversation?\\ncommon, just try\\nyou suck\\nbots are bad\\ni dont like bots\\nyou instruct me very much\\nohh! i am confused.\\nwhat's the weather today\\nno wait go back i want a dripping ice cream but a cone that catches it so you  \\ncan drink the ice cream later\\ni want a non dripping ice cream\\nshow me a picture of a chicken\\nI want french cuisine\\nrestaurants\\nrestaurant\\ncan i be shown a gluten free restaurant\\nagain?\\noh wait i gave you my work email address can i change it?\\nstop it, i do not care!!!\\n how come?\\ndid i break you\\nI don't wanna tell the name of my company\\nthat link doesn't work!\\nyou already have that\\nthis is a really frustrating experience\\ni want a french restaurant\\nsilly bot\\ni want to eat\\ni hate you\\nCan I ask you questions first?\\nis it a wasteland full of broken robot parts?\\ni can't deal with your request\\nwho will anser my email?\\nand make chicken noises into the phone\\nwhat's your wife doing this weekend\\nhow are the kids\\nyou're rather dull\\npersonal or work?\\nare you using Rasa Core and NLU ?\\nwhat else?\\nI already told you! I'm a shitmuncher\\nI'm a shitmuncher\\nwho are the engineers at rasa?\\nwho are they?\\nwho is your favourite robot?\\na tamed mouse will arrive at your doorstep in the next couple of days\\nyou will know it from the single red rose it carries between its teeth\\ni will tame a mouse for you\\nisn't the newsletter just spam?\\ncan you help me with the docs?\\nsorry, i cannot rephrase\\nand your REST API doesn't work\\ni told you already\\nbetter than you\\nyou are a badass bot!\\nwhy do you need that?\\nis that any of your business\\ncan you help me with your docs?\\ni immediately need help with implementing the coolest bot you can imagine\\ncan you help me with your docs\\nhey, I contacted you a couple of days ago but didn't get any response, any news?\\nplease hurry, i have deadline in two weeks to deliver the bot it is for very big company\\nDo I have to accept?\\nIs Rasa really smart?\\nkannst du auch deutsch?\\nare the newsletter worth the subscription?\\nit's a pity\\ni want more of you in my life!\\nthe one that is better than you\\ndo you have a phone number?\\nhow are akela's cats doing?\\nbut I just told you that :(\\nWhy don’t you answer?\\nBut you're an english site :(\\ncan you help me to build a bot\\nno\\ndefinitely not\\nnever\\nabsolutely not\\ni don't think so\\ni'm afraid not\\nno sir\\nno ma'am\\nno way\\nno sorry\\nNo, not really.\\nnah not for me\\nnah\\nno and no again\\nno thanks\\ndecline\\ndeny\\ni decline\\nno i don't accept\\nno you did it wrong\\nnein\\ni don't want to\\ni don't want either of those\\nnah thanks\\nneither of these\\nI'm not giving you my employee Id\\nI'm not giving you my empId\\nno I haven't decided yet if I want to do so\\nI don't want to give it to you\\nI'm not going to give it to you.\\nWhat is WBT?\\nWhat do you mean by WBT?\\nI don't know who the WBT is\\nCan you tell me what the WBT is\\nWBT?\\nWhat's WBT?\\nI am unaware of WBT\\nI am not aware of WBT\\nWeb Based Training?\\nCan you explain me WBT\\nwhat is wbt ?\\nwhat is wbt\\nwbt \\nwbt meaning\\nweb based training\\nwhat's wbt\\nlia what is wbt ?\\nwhat is wbt lia ? \\ncan you tell me about wbt ?\\nwbt means ?\\nwbt means \\nweb based training\\nweb based training meaning\\nweb based training means\\ntell me what you know about wbt\\nlia tell me what you know about wbt\\nWhat are new courses?\\nnew courses?\\nWhat do you mean by new courses?\\nCan you tell me what new courses is ?\\nWhat's new courses?\\nI am unaware of new courses\\nI am not aware of new courses\\nCan you explain me new courses ?\\nwhat are the new courses\\nnew courses means\\nnew courses\\nlia what is new courses ?\\nlia what are new courses\\ncan you tell me about new courses ? \\nnew courses meaning \\nnew courses means ?\\nlia what do you mean by new courses ?\\nlia what do you mean by new courses\\ntell me what you know about new courses\\nlia tell me what you know about new courses\\nWhat are new competencies?\\nnew competencies?\\nWhat do you mean by new competencies?\\nCan you tell me what new competencies is ?\\nWhat's new competencies?\\nI am unaware of new competencies\\nI am not aware of new competencies\\nCan you explain me new competencies ?\\nlia what do you mean by new competencies ? \\nwhat are new competencies lia ?\\nnew competencies mean ? \\nnew competencies means\\ncan you explain me about new competencies lia ? \\ncan you tell me about new competencies\\ncan you tell me about new competencies ? \\nexplain more about new competencies\\nnew competencies meaning \\nlia what are new competencies\\nlia what are new competencies ?\\nnew competencies \\nwhat are the new competencies lia ?\\ntell me what you know about new competencies\\nlia tell me what you know about new competencies\\nWhat are popular competencies?\\npopular competencies?\\nWhat do you mean by popular competencies?\\nCan you tell me what popular competencies is ?\\nWhat's popular competencies?\\nI am unaware of popular competencies\\nI am not aware of popular competencies\\npopular competencies\\nlia what are popular competencies ?\\nlia what do you mean by popular competencies ?\\nCan you explain me popular competencies ?\\npopular competencies meaning\\npopular competencies meaning ?\\ncan you tell me about popular competencies ?\\ncan you explain about popular competencies ?\\ncan you tell me about the popular competencies\\npopular competencies means ?\\npopukar competencies means\\nwhat is popular competencies\\ncan you tell me more about popular competencies\\ncan you tell me more about popular competencies\\nexplain popular competencies\\npopular competencies \\nexplain popular competencies\\nelaborate popular competencies\\nelaborate on popular competencies\\nelaborate on the popular competencies lia\\ntell me what you know about popular competencies\\nlia tell me what you know about popular competencies\\nWhat are mandatory courses?\\nmandatory courses?\\nWhat do you mean by mandatory courses?\\nCan you tell me what mandatory courses is ?\\nWhat's mandatory courses?\\nI am unaware of mandatory courses\\nI am not aware of mandatory courses\\nmandatory courses\\nlia what are mandatory courses ?\\nlia what do you mean by mandatory courses ?\\nCan you explain me mandatory courses ?\\nmandatory courses meaning\\nmandatory courses meaning ?\\ncan you tell me about mandatory courses ?\\ncan you explain about mandatory courses ?\\ncan you tell me about the mandatory courses\\nmandatory courses means ?\\nmandatory courses means\\nwhat is mandatory courses\\ncan you tell me more about mandatory courses\\ncan you tell me more about mandatory courses\\nexplain mandatory courses\\nmandatory courses \\nexplain mandatory courses\\nelaborate mandatory courses\\nelaborate on mandatory courses\\nelaborate on the mandatory courses lia\\ntell me what you know about mandatory courses\\nlia tell me what you know about mandatory courses\\nWhat are popular courses?\\npopular courses?\\nWhat do you mean by popular courses?\\nCan you tell me what popular courses is ?\\nWhat's popular courses?\\nI am unaware of popular courses\\nI am not aware of popular courses\\npopular courses\\nlia what are popular courses ?\\nlia what do you mean by popular courses ?\\nCan you explain me popular courses ?\\npopular courses meaning\\npopular courses meaning ?\\ncan you tell me about popular courses ?\\ncan you explain about popular courses ?\\ncan you tell me about the popular courses\\npopular courses means ?\\npopular courses means\\nwhat is popular courses\\ncan you tell me more about popular courses\\ncan you tell me more about popular courses\\nexplain popular courses\\nexplain popular courses\\nelaborate popular courses\\nelaborate on popular courses\\nelaborate on the popular courses lia\\ntell me what you know about popular courses\\nlia tell me what you know about popular courses\\nWhat are ILT courses?\\nWhat are ilt courses?\\nILT courses?\\nWhat do you mean by ILT courses?\\nCan you tell me what ILT courses is ?\\nWhat's ILT courses?\\nI am unaware of ILT courses\\nI am not aware of ILT courses\\nILT courses\\nlia what are ILT courses ?\\nlia what do you mean by ILT courses ?\\nCan you explain me ILT courses ?\\nILT courses meaning\\nILT courses meaning ?\\ncan you tell me about ILT courses ?\\ncan you explain about ILT courses ?\\ncan you tell me about the ILT courses\\nILT courses means ?\\nILT courses means\\nwhat is ILT courses\\ncan you tell me more about ILT courses\\ncan you tell me more about ILT courses\\nexplain ILT courses\\nexplain ILT courses\\nelaborate ILT courses\\nelaborate on ILT courses\\nelaborate on the ILT courses lia\\ntell me what you know about ILT courses\\nlia tell me what you know about ILT courses\\nWhat is mean by sme\\nWhat are SME?\\ntell me about SME\\nWhat do you mean by SME?\\nCan you tell me what is SME ?\\nCan you tell me what is  mean by sme ?\\nWhat's sme?\\nI am unaware about sme\\nI am not aware about SME\\nlia what is mean by sme ?\\nlia what do you mean by sme?\\nCan you explain me about sme ?\\nsme meaning\\nSME meaning ?\\ncan you tell me about sme?\\ncan you explain about SME?\\ncan you tell me about the sme\\nsme means ?\\nSME means\\nwhat is SME\\ncan you tell me more about sme\\ncan you tell me more about sme\\nexplain sme\\nelaborate sme\\nelaborate on sme\\nelaborate on the sme lia\\ntell me what you know about SME\\nlia tell me what you know about sme\\nWhat is about course? \\nWhat do you mean by about course?\\nI don't know what the about course is\\nCan you tell me what the about course is\\nabout course?\\nWhat's about course?\\nI am unaware of about course\\nI am not aware of about course\\nCan you explain me about course\\nabout course meaning\\nabout course means ?\\nabout course means ?\\nabout course means\\nabout course meaning\\ncan you tell me what about course is\\ncan you tell me what about course means\\neloborate on about course\\ngist of about course\\nlia what is about course \\nlia what does about course means\\nlia what does about course mean ?\\nwhat does about course mean lia ?\\nabout course\\ntell me what you know about about course\\nlia tell me what you know about about course\\nWhat are prerequisites?\\nprerequisites?\\nWhat do you mean by prerequisites?\\nCan you tell me what prerequisites is ?\\nWhat's prerequisites?\\nI am unaware of prerequisites\\nI am not aware of prerequisites\\nprerequisites\\nlia what are prerequisites ?\\nlia what do you mean by prerequisites ?\\nCan you explain me prerequisites ?\\nprerequisites meaning\\nprerequisites meaning ?\\ncan you tell me about prerequisites ?\\ncan you explain about prerequisites ?\\ncan you tell me about the prerequisites\\nprerequisites means ?\\nprerequisites means\\nwhat is prerequisites\\ncan you tell me more about prerequisites\\ncan you tell me more about prerequisites\\nexplain prerequisites\\nprerequisites   \\nexplain prerequisites\\nelaborate prerequisites\\nelaborate on prerequisites\\nelaborate on the prerequisites lia\\ntell me what you know about prerequisites\\nlia tell me what you know about prerequisites\\nWhat is enroll?\\nWhat do you mean by enroll?\\nI don't know what the enroll is\\nCan you tell me what the enroll is\\nenroll?\\nWhat's enroll?\\nI am unaware of enroll\\nI am not aware of enroll\\nCan you explain me enroll\\nenroll a course meaning\\nenroll a course means ?\\nenroll means ?\\nenroll means\\nenroll meaning\\ncan you tell me what enroll is\\ncan you tell me what enroll means\\neloborate on enroll\\ngist of enroll\\nlia what is enroll \\nlia what does enroll a course means\\nlia what does enroll mean ?\\nwhat does enroll mean lia ?\\nenroll\\ntell me what you know about enroll\\nlia tell me what you know about enroll\\nWhat is launch?\\nWhat do you mean by launch?\\nI don't know what the launch is\\nCan you tell me what the launch is\\nlaunch?\\nWhat's launch?\\nI am unaware of launch\\nI am not aware of launch\\nCan you explain me launch\\nlia what is launch ?\\nlia what is launch\\nlia what's launch\\nelaborate on launch lia\\ncan you explain me what launch is \\ncan you tell me what a launch is \\ntell me what a launch is \\ntell me what launch means\\nlaunch meaning\\nlaunch meaning ?\\nlaunch means\\nlaunch a course means?\\nlaunch a course meaning ? \\nlaunch course\\ntell me what you know about launch\\nlia tell me what you know about launch\\nWhat is drop?\\nWhat do you mean by drop?\\nI don't know what the drop is\\nCan you tell me what the drop is\\ndrop?\\nWhat's drop?\\nI am unaware of drop\\nI am not aware of drop\\nCan you explain me drop\\nlia what is drop ?\\nlia what is drop\\nlia what's drop\\nelaborate on drop lia\\ncan you explain me what drop is \\ncan you tell me what a drop is \\ntell me what a drop is \\ntell me what drop means\\ndrop meaning\\ndrop meaning ?\\ndrop means\\ndrop a course means?\\ndrop a course meaning ? \\ndrop course\\ntell me what you know about drop\\nlia tell me what you know about drop\\nwhat is mean by competency\\nWhat are competencies?\\ncompetencies?\\nWhat do you mean by competencies?\\nCan you tell me what competencies is ?\\nWhat's competencies?\\nI am unaware of competencies\\nI am not aware of competencies\\ncompetencies\\nlia what are competencies ?\\nlia what do you mean by competencies ?\\nCan you explain me competencies ?\\ncompetencies meaning\\ncompetencies meaning ?\\ncan you tell me about competencies ?\\ncan you explain about competencies ?\\ncan you tell me about the competencies\\ncompetencies means ?\\ncompetencies means\\nwhat is competencies\\ncan you tell me more about competencies\\ncan you tell me more about competencies\\nexplain competencies\\ncompetencies \\nexplain competencies\\nelaborate competencies\\nelaborate on competencies\\nelaborate on the competencies lia\\ntell me what you know about competencies\\nlia tell me what you know about competencies\\nWhat are Expired courses?\\nExpired courses?\\nWhat do you mean by expired courses?\\nCan you tell me what expired courses is ?\\nWhat's expired courses?\\nI am unaware of expired courses\\nI am not aware of expired courses\\nexpired courses\\nlia what are expired courses ?\\nlia what do you mean by expired courses ?\\nCan you explain me expired courses ?\\nexpired courses meaning\\nexpired courses meaning ?\\ncan you tell me about expired courses ?\\ncan you explain about expired courses ?\\ncan you tell me about the expired courses\\nexpired courses means ?\\nexpired courses means\\nwhat is expired courses\\ncan you tell me more about expired courses\\ncan you tell me more about expired courses\\nexplain expired courses\\nexplain expired courses\\nelaborate expired courses\\nelaborate on expired courses\\nelaborate on the expired courses lia\\ntell me what you know about expired courses\\nlia tell me what you know about expired courses\\nWhat are reviews?\\nreviews?\\nWhat do you mean by reviews?\\nCan you tell me what reviews is ?\\nWhat's reviews?\\nI am unaware of reviews\\nI am not aware of reviews\\nreviews\\nlia what are reviews ?\\nlia what do you mean by reviews ?\\nCan you explain me reviews ?\\nreviews meaning\\nreviews meaning ?\\ncan you tell me about reviews ?\\ncan you explain about reviews ?\\ncan you tell me about the reviews\\nreviews means ?\\nreviews means\\nwhat is reviews\\ncan you tell me more about reviews\\ncan you tell me more about reviews\\nexplain reviews\\nreviews \\nexplain reviews\\nelaborate reviews\\nelaborate on reviews\\nelaborate on the reviews lia\\ntell me what you know about reviews\\nlia tell me what you know about reviews\\nWhat are notifications?\\nnotifications?\\nWhat do you mean by notifications?\\nCan you tell me what notifications is ?\\nWhat's notifications?\\nI am unaware of notifications\\nI am not aware of notifications\\nnotifications\\nlia what are notifications ?\\nlia what do you mean by notifications ?\\nCan you explain me notifications ?\\nnotifications meaning\\nnotifications meaning ?\\ncan you tell me about notifications ?\\ncan you explain about notifications ?\\ncan you tell me about the notifications\\nnotifications means ?\\nnotifications means\\nwhat is notifications\\ncan you tell me more about notifications\\ncan you tell me more about notifications\\nexplain notifications\\nnotifications \\nexplain notifications\\nelaborate notifications\\nelaborate on notifications\\nelaborate on the notifications lia\\ntell me what you know about notifications\\nlia tell me what you know about notifications\\nWhat are page search?\\npage search?\\nWhat do you mean by page search?\\nCan you tell me what page search is ?\\nWhat's page search?\\nI am unaware of page search\\nI am not aware of page search\\npage search\\nlia what are page search ?\\nlia what do you mean by page search ?\\nCan you explain me page search ?\\npage search meaning\\npage search meaning ?\\ncan you tell me about page search ?\\ncan you explain about page search ?\\ncan you tell me about the page search\\npage search means ?\\npage search means\\nwhat is page search\\ncan you tell me more about page search\\ncan you tell me more about page search\\nexplain page search\\npage search \\nexplain page search\\nelaborate page search\\nelaborate on page search\\nelaborate on the page search lia\\ntell me what you know about page search\\nlia tell me what you know about page search\\nWhat are learning points?\\nlearning points?\\nWhat do you mean by learning points?\\nCan you tell me what learning points is ?\\nWhat's learning points?\\nI am unaware of learning points\\nI am not aware of learning points\\nlearning points\\nlia what are learning points ?\\nlia what do you mean by learning points ?\\nCan you explain me learning points ?\\nlearning points meaning\\nlearning points meaning ?\\ncan you tell me about learning points ?\\ncan you explain about learning points ?\\ncan you tell me about the learning points\\nlearning points means ?\\nlearning points means\\nwhat is learning points\\ncan you tell me more about learning points\\ncan you tell me more about learning points\\nexplain learning points\\nlearning points \\nexplain learning points\\nelaborate learning points\\nelaborate on learning points\\nelaborate on the learning points lia\\ntell me what you know about learning points\\nlia tell me what you know about learning points\\nWhat are course feedback?\\ncourse feedback?\\nWhat do you mean by course feedback?\\nCan you tell me what course feedback is ?\\nWhat's course feedback?\\nI am unaware of course feedback\\nI am not aware of course feedback\\ncourse feedback\\nlia what are course feedback ?\\nlia what do you mean by course feedback ?\\nCan you explain me course feedback ?\\ncourse feedback meaning\\ncourse feedback meaning ?\\ncan you tell me about course feedback ?\\ncan you explain about course feedback ?\\ncan you tell me about the course feedback\\ncourse feedback means ?\\ncourse feedback means\\nwhat is course feedback\\ncan you tell me more about course feedback\\ncan you tell me more about course feedback\\nexplain course feedback\\ncourse feedback \\nexplain course feedback\\nelaborate course feedback\\nelaborate on course feedback\\nelaborate on the course feedback lia\\nfeedback on the course means\\nfeedback on course meaning\\nfeedback on course means ?\\nfeedback on the course?\\ntell me what you know about course feedback\\nlia tell me what you know about course feedback\\nWhat are delivery modes?\\ndelivery modes?\\nWhat do you mean by delivery modes?\\nCan you tell me what delivery modes is ?\\nWhat's delivery modes?\\nI am unaware of delivery modes\\nI am not aware of delivery modes\\ndelivery modes\\nlia what are delivery modes ?\\nlia what do you mean by delivery modes ?\\nCan you explain me delivery modes ?\\ndelivery modes meaning\\ndelivery modes meaning ?\\ncan you tell me about delivery modes ?\\ncan you explain about delivery modes ?\\ncan you tell me about the delivery modes\\ndelivery modes means ?\\ndelivery modes means\\nwhat is delivery modes\\ncan you tell me more about delivery modes\\nexplain delivery modes\\ndelivery modes \\nexplain delivery modes\\nelaborate delivery modes\\nelaborate on delivery modes\\nelaborate on the delivery modes lia\\nwhat are the various types of delivery modes\\nwhat are the different types of delivery modes lia?\\nwhat are the different types of delivery modes ?\\ndifferent types of delivery modes ?\\nwhat are the types delivery modes in which I can take up courses ?\\nlia what are the types delivery modes in which I can take up courses ?\\ncan you tell me about delivery modes  in which I can take up courses ?\\ntell me what you know about delivery modes\\nlia tell me what you know about delivery modes\\nWhat are course overview?\\ncourse overview?\\nWhat do you mean by course overview?\\nCan you tell me what course overview is ?\\nWhat's course overview?\\nI am unaware of course overview\\nI am not aware of course overview\\nCan you explain me course overview ?\\nwhat are the course overview\\ncourse overview means\\ncourse overview\\nlia what is course overview ?\\nlia what are course overview\\ncan you tell me about course overview ? \\ncourse overview meaning \\ncourse overview means ?\\nlia what do you mean by course overview ?\\nlia what do you mean by course overview\\nlia what do you mean by overview of the course ?\\nlia what does overview of the course mean?\\ntell me what you know about course overview\\nlia tell me what you know about course overview\\nI want career path for rasa please suggest\\nget carrier guidence on\\nwhat are courses on it\\nshow me the details of those employee who are skilled on this topic\\ncan you get me employee details who are trained on this  technology\\nget me upcoming sessions\\ni want to know about the session details\\ni want session details\\nplease show me details of the session \\nwhich session i have to do after this session?\\nany more session?\\nany sessions?\\nwhat are sessions on it\\ncan you get me sessions on it\\nwhat are sessions on this\\nget me course status\\nI want to talk to a experties?\\nwho is SME\\nI want to talk to a SME\\nI want to talk to a experties\\ni want to disscuss with a SME\\ni want to disscuss my doubt with a SME\\nlet me speak with a SME please\\ngimme a SME\\ngive a SME\\ngive a SME\\nCan I speak to anyone who can really help me?\\ngive me a SME now\\ni would like to speak to a SME\\ncan you put me in touch with a SME\\nlet me talk to a SME\\ncan i please speak to a SME\\ni want to speak to a SME\\ni want to speak to a SME for this\\ncall experties for this\\nis there any expired courses\\ni need expired courses\\ndo you have expired courses\\ni am searching for expired courses\\nany popular courses to show?\\nwhich are courses most suggested by expertise?\\nwhich are courses having most highest ratings?\\ndo we have any popular course added ?\\nany recommendation of the courses for me?\\ngive me some popular course\\nlet me see some popular courses\\nwhat are trending courses right now?\\nwhats new in market?\\nget popular courses for me.\\nget popular courses for it.\\nshow me list of ilt courses\\nget me some ilt courses\\nany ilt course?\\ni am searching for ilt courses\\nwhat are ILT courses\\nshow me list of vilt courses\\nget me some vilt courses\\nany vilt course?\\ni am searching for vilt courses\\nwhat are vilt courses\\nwhat are vilt courses\\nany VILT course?\\nshow me list of VILT courses\\nget me some VILT courses\\ni am searching for VILT courses\\nwhat are VILT courses\\nwhat are VILT courses\\nwhat are vlit courses\\nget me some virtual instructor lead courses on this topic\\ndo you have any new course?\\ndo you have any new course added to your list\\nshow me new course added to your course list.\\nany new course added?\\ngive me some new courses that are added \\ndo we have any new course added ?\\ni want to see new courses that are recently added\\ngive me some new courses\\nget me some new courses on this topic\\nSuggest me courses \\nsuggest me some courses\\nPlease Suggest me courses \\nSuggest me some courses \\nPlease Suggest me courses now \\nSuggest me some courses \\nPlease Suggest any courses \\nSuggest the some courses \\nSuggest courses for me\\nPlease Suggest the courses\\nOffer me courses now\\nPlease Offer me some courses\\nOffer me any courses \\nPlease Offer me some courses \\nOffer  courses for me now\\nPlease Offer me some courses\\nOffer the courses  \\nOffer any courses for me\\nPlease Offer the courses \\nPlease Offer any courses \\nOffer the courses \\nPlease Recommend me some courses \\nRecommend me courses \\nPlease Recommend me some courses \\nPlease Recommend the courses for me\\nPlease Recommend any courses\\nRecommend the courses now\\nsick leave\\nflexi leaves\\nI want to quit the job, I am being harassed.\\nI want to pursue higher studies , I need to quit my job.\\nI want to quit the job, I have serious health issues.\\nUnable to balance work life environment, I would like to quit the job. \\nI am being harassed in my workplace .\\nI am pregnant , I want to quit my job.\\nI am feeling stressed lately, want to quit the job .\\nI want to quit the job .\\nI quit .\\nWork culture is not good , I want to quit .\\nIn need of a career change , I want to quit .\\nI hate my job , I want to quit immediately .\\nI no longer want  to continue in this organisation.I quit.\\nI am unable to work on shifts. I am quitting.\\nI am unable to cope up with shift timings as my home is far away. I want to quit.\\nI am being verbally abused by my TL, I want to file a complaint.\\nI am being  verbally abused by my manager, I want to quit my job.\\nI am being verbally abused by my collegue, I want to raise a complaint.\\nMy Manager is abusing me, I want to raise an incident.\\nI am verbally abused in my workplace.\\nI am facing sexual harassment.\\nI am being physically abused by my manager . \\nThe work environment is toxic, I want to quit.\\nI am being discriminated due to my gender, I want to quit.\\nI am being abused as I belong to LGBT community, I want to quit my job.\\nI am being discriminated by caste, I want to quit.\\nI am being harassed by my bus driver, I want to raise a issue.\\nI am thinking of quitting the job.\\nI want to complaint about the workplace harassment.\\nFile a complaint on harassment.\\nMy GYM trainer is harassing me, I want to raise a complaint.\\nI am constantly being harassed by my team mates, I want to contact the HR.\\nI want to contact the HR to complain about workplace harassment.\\nMy team mate assaulted me, I want to contact HR immediately.\\nI am being discriminated for my religion at my workplace .\\nI am underpaid at my job , I want to quit my job .\\nJob feels pointless , I wish to quit .\\nI want a change in career direction , I want to contact HR .\\nWanting to change career path , I want to quit .\\ndo you know what type of leaves here\\nwhat type of leaves in tcs\\ntcs has what type of leaves\\nwhat type of leaves TCS have?\\nwhat are the leaves type\\nwhat are types of leaves\\ndo you know what type of leaves TCS have\\nWhat type of leaves we have?\\ndo you know what type of leaves we have?\\nwhat are leave types\\nwhat kind of leaves we have\\nwhat are the leave types\\nWhere can I apply leave from?\\nwhere can i apply for leave\\nfrom where can i apply leaves\\ncan you tell me where can I apply leave from?\\ncan you show me where can I apply leave from?\\ncan you tell me where can i apply leave from \\ncan you show me from where can i apply leave from\\nhow to apply for leave\\nhow to apply for sick leave\\ndo you know how to apply for sick leave\\ndo you know how to apply for leave\\nproceed\\nPROCEED\\nProceed\\nlet's proceed\\nya, proceed now\\nproceed now\\nproceed\\nok proceed  \\nok proceed it!\\nI want to cancel my leave. How can I do that?\\ncancel my leave request\\nI want to cancel my leave request\\ncan you cancel my leave request\\ncancel my sick leaves request\\ncancel my flexi leave\\ndelete my leave request\\nis there any way to cancel my leave request\\ni want to apply leave\\ni want to apply sick leave\\napply 3 days leave for me\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ1jQ5nJnQ16"
      },
      "source": [
        "data = file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itdAHSs0nbJr"
      },
      "source": [
        "corpus = data.lower().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heWJRfVZnfnj"
      },
      "source": [
        "corpus = [x.strip() for x in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBt9pGcIqGVF"
      },
      "source": [
        "### Cleaning Corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffvU07oJoT-5"
      },
      "source": [
        "def clean_special_chars(text, punct):\n",
        "    for p in punct:\n",
        "        text = text.replace(p, ' ')\n",
        "    return text\n",
        "\n",
        "      \n",
        "def preprocess(data):\n",
        "    output = []\n",
        "    punct = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    for line in data:\n",
        "         pline= clean_special_chars(line, punct)\n",
        "         output.append(pline)\n",
        "    return output  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlYREj1o4Nn"
      },
      "source": [
        "corpus = preprocess(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swzBj6MdpnxX",
        "outputId": "fb1c06ea-5156-4ba6-f84c-221042510766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1664"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MoM3GWSoIeN"
      },
      "source": [
        "corpus = [x for x in corpus if len(x.split())>1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNhEQudIps3b",
        "outputId": "5028a393-32c1-4915-ab27-2a1ff51d810e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHIk8IE5pwRq"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FreTXfZ3qXs6",
        "outputId": "1a342a12-cc0b-4d04-eca6-11bb8c6948e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl6N42w6qals"
      },
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T51Xp6BOqgVM"
      },
      "source": [
        "tokenizer.fit_on_texts(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzwxPZXaqi58"
      },
      "source": [
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmOWVLwwqmsj",
        "outputId": "859135a8-2597-46f5-a324-489a6aaf3b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuSk7BO2qtrN"
      },
      "source": [
        "input_sequences = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt2qppRvqyON"
      },
      "source": [
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6C3F_ZJq1RF",
        "outputId": "6cb823a1-aff3-48d5-9b19-c94e0755c34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdak6l8Yq4BE",
        "outputId": "b8e956ad-4e59-4fad-bd57-d43ef00eaa3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_sequences[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[108, 59],\n",
              " [116, 59],\n",
              " [68, 205],\n",
              " [108, 206],\n",
              " [108, 377],\n",
              " [108, 378],\n",
              " [108, 379],\n",
              " [108, 117],\n",
              " [124, 380],\n",
              " [124, 10],\n",
              " [124, 10, 381],\n",
              " [124, 10, 381, 117],\n",
              " [124, 245],\n",
              " [108, 108],\n",
              " [116, 116],\n",
              " [124, 117],\n",
              " [116, 59],\n",
              " [124, 207],\n",
              " [68, 205],\n",
              " [124, 294],\n",
              " [124, 294, 382],\n",
              " [116, 117],\n",
              " [124, 383],\n",
              " [384, 385],\n",
              " [116, 59],\n",
              " [146, 97],\n",
              " [295, 124],\n",
              " [295, 124, 117],\n",
              " [4, 178],\n",
              " [4, 178, 386],\n",
              " [146, 97],\n",
              " [146, 97, 33],\n",
              " [146, 97, 33, 59],\n",
              " [116, 246],\n",
              " [116, 246, 132],\n",
              " [116, 246],\n",
              " [116, 246, 132],\n",
              " [116, 387],\n",
              " [124, 41],\n",
              " [124, 41, 10],\n",
              " [124, 41, 10, 3],\n",
              " [124, 41, 10, 3, 206],\n",
              " [2, 97],\n",
              " [108, 117],\n",
              " [108, 207],\n",
              " [108, 117],\n",
              " [108, 117, 156],\n",
              " [108, 117, 156, 3],\n",
              " [68, 296],\n",
              " [68, 205],\n",
              " [68, 388],\n",
              " [80, 68],\n",
              " [4, 19],\n",
              " [4, 19, 247],\n",
              " [4, 19, 247, 80],\n",
              " [4, 19, 247, 80, 68],\n",
              " [4, 19],\n",
              " [4, 19, 133],\n",
              " [109, 68],\n",
              " [80, 248],\n",
              " [80, 157],\n",
              " [36, 80],\n",
              " [36, 80, 68],\n",
              " [297, 248],\n",
              " [49, 248],\n",
              " [7, 1],\n",
              " [7, 1, 24],\n",
              " [7, 1, 24, 59],\n",
              " [7, 1],\n",
              " [7, 1, 24],\n",
              " [7, 1, 24, 298],\n",
              " [19, 4],\n",
              " [19, 4, 179],\n",
              " [19, 4, 179, 11],\n",
              " [19, 4, 179, 11, 24],\n",
              " [19, 4, 179, 11, 24, 59],\n",
              " [19, 4],\n",
              " [19, 4, 179],\n",
              " [19, 4, 179, 11],\n",
              " [19, 4, 179, 11, 24],\n",
              " [19, 4, 179, 11, 24, 298],\n",
              " [134, 1],\n",
              " [134, 1],\n",
              " [134, 1, 49],\n",
              " [134, 1, 49, 180],\n",
              " [87, 59],\n",
              " [87, 42],\n",
              " [87, 42, 69],\n",
              " [389, 390],\n",
              " [135, 87],\n",
              " [299, 134],\n",
              " [299, 134, 1],\n",
              " [87, 24],\n",
              " [87, 24, 300],\n",
              " [87, 24, 300, 42],\n",
              " [87, 24, 300, 42, 158],\n",
              " [87, 42],\n",
              " [87, 42, 13],\n",
              " [87, 42, 13, 43],\n",
              " [87, 24]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIIaoaihq6b0"
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82tswkZwrc3J",
        "outputId": "ece5241f-5bfa-4b8f-949e-0acab835ab18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_sequence_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAZyyqX0rfkn",
        "outputId": "979f30e1-46f5-4d58-98c1-2e778c8a70b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpwsOFOQrlVx"
      },
      "source": [
        "input_sequences = np.array(pad_sequences(input_sequences,   \n",
        "                          maxlen=max_sequence_len, padding='pre'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIB4K1n9rn0I",
        "outputId": "4355386a-02a5-4fca-a1d4-c1b6e708e108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input_sequences[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108,  59],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 116,  59],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  68, 205],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108, 206],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108, 377]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aBOVknyrrQn"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7g2znq1rvtw"
      },
      "source": [
        "with open('/content/drive/My Drive/WordPrediction/input_sequences_bot.pickle', 'wb') as inp_seq:\n",
        "    pickle.dump(input_sequences, inp_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXHbMjYCr2U4"
      },
      "source": [
        "input_sequences = pickle.load(open(\"/content/drive/My Drive/WordPrediction/input_sequences_bot.pickle\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFaNR6m9r7Qp",
        "outputId": "e7ad3ddd-ce24-4b59-cdc2-9c651062eba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(input_sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3NU-Mkyr-Lo"
      },
      "source": [
        "import keras.utils as ku"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-nIiLlwsApg"
      },
      "source": [
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ7P5h39sFES"
      },
      "source": [
        "with open('/content/drive/My Drive/WordPrediction/predictors_bot.pickle', 'wb') as predictor:\n",
        "    pickle.dump(predictors, predictor)\n",
        "\n",
        "with open('/content/drive/My Drive/WordPrediction/label_bot.pickle', 'wb') as lab:\n",
        "    pickle.dump(label, lab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiQpxj4nsMxZ"
      },
      "source": [
        "predictors = pickle.load(open(\"/content/drive/My Drive/WordPrediction/predictors_bot.pickle\", \"rb\" ) )\n",
        "label = pickle.load(open(\"/content/drive/My Drive/WordPrediction/label_bot.pickle\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0jIwbgnsTJL",
        "outputId": "a282b45f-9e24-4d7c-81ae-d63537322c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6166, 718)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K9JK7kcsWNR",
        "outputId": "77d58abb-3df8-45bf-83e4-7160a9b5d39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6166, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xYkeB96sgZt",
        "outputId": "01f93164-b0bc-4ee5-dacb-b1a175e7a11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "predictors[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 116],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  68],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 108]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjdURmrJsjPn",
        "outputId": "2006e545-af80-4c20-b907-87a4a87c6e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "label[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTKl9jPbsYfK"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il74TkK3srU-",
        "outputId": "bcaa09c4-9c7d-4789-f91c-5f2d07d4ba30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/My Drive/WordPrediction/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvxKQ4g0szsE"
      },
      "source": [
        "with open('/content/drive/My Drive/WordPrediction/embeddings_index_glove6B100d.pickle', 'wb') as emb_idx:\n",
        "    pickle.dump(embeddings_index, emb_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIN5g5dTs9Mk"
      },
      "source": [
        "embeddings_index = pickle.load(open(\"/content/drive/My Drive/WordPrediction/embeddings_index_glove6B100d.pickle\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqKAMs9MtBKU",
        "outputId": "1b8bbafe-83c5-4bc2-aad7-280eae2de098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(embeddings_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuJ_Ol9NtD0T",
        "outputId": "4ab078f8-bdaf-4e46-eb8a-0d76f834f549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfMwl1xdtGhN"
      },
      "source": [
        "vocab_size = total_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7UzKjV5tJLc"
      },
      "source": [
        "# create a weight matrix for words in training corpus\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IulsoU8StQFc",
        "outputId": "2bbb4e19-edcf-47b2-e632-b6c980632c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer.word_index.items()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('you', 1), ('what', 2), ('me', 3), ('i', 4), ('can', 5), ('courses', 6), ('are', 7), ('tell', 8), ('about', 9), ('is', 10), ('to', 11), ('course', 12), ('the', 13), ('lia', 14), ('do', 15), ('your', 16), ('how', 17), ('of', 18), ('am', 19), ('want', 20), ('competencies', 21), ('explain', 22), ('know', 23), ('a', 24), ('popular', 25), ('by', 26), ('means', 27), ('mean', 28), ('on', 29), ('who', 30), ('new', 31), ('were', 32), ('my', 33), ('more', 34), ('meaning', 35), ('not', 36), ('elaborate', 37), (\"what's\", 38), ('cancel', 39), ('sme', 40), ('it', 41), ('for', 42), ('help', 43), ('delivery', 44), ('modes', 45), ('where', 46), ('ilt', 47), ('feedback', 48), ('so', 49), ('expired', 50), (\"you're\", 51), ('mandatory', 52), ('page', 53), ('search', 54), ('learning', 55), ('points', 56), ('have', 57), ('please', 58), ('bot', 59), ('this', 60), ('prerequisites', 61), ('reviews', 62), ('notifications', 63), ('in', 64), ('from', 65), ('launch', 66), ('drop', 67), ('good', 68), ('that', 69), ('unaware', 70), ('aware', 71), ('enroll', 72), ('leave', 73), ('activity', 74), ('any', 75), ('overview', 76), ('quit', 77), ('with', 78), ('look', 79), ('very', 80), ('no', 81), ('some', 82), ('wbt', 83), (\"don't\", 84), ('now', 85), ('answer', 86), ('thanks', 87), ('give', 88), ('be', 89), ('get', 90), ('annoying', 91), ('made', 92), ('did', 93), ('bye', 94), ('really', 95), ('work', 96), ('up', 97), ('which', 98), ('age', 99), ('will', 100), ('job', 101), ('apply', 102), ('being', 103), ('just', 104), ('beautiful', 105), ('boring', 106), ('leaves', 107), ('hi', 108), (\"i'm\", 109), ('and', 110), ('need', 111), ('vilt', 112), ('show', 113), ('suggest', 114), ('offer', 115), ('hey', 116), ('there', 117), ('like', 118), ('old', 119), ('birthday', 120), ('today', 121), ('hungry', 122), ('request', 123), ('hello', 124), ('created', 125), ('was', 126), ('day', 127), ('company', 128), ('does', 129), ('pretty', 130), ('types', 131), ('talk', 132), ('great', 133), ('thank', 134), ('ok', 135), ('come', 136), ('born', 137), ('birth', 138), ('think', 139), ('else', 140), ('boss', 141), ('question', 142), ('looking', 143), ('forget', 144), ('type', 145), ('whats', 146), ('when', 147), ('city', 148), ('home', 149), ('going', 150), ('doing', 151), ('stop', 152), ('information', 153), ('marry', 154), ('added', 155), (\"it's\", 156), ('bad', 157), ('everything', 158), ('cool', 159), ('as', 160), ('way', 161), ('build', 162), ('u', 163), ('last', 164), ('been', 165), ('at', 166), ('name', 167), ('hometown', 168), (\"how's\", 169), ('but', 170), ('eat', 171), ('bored', 172), ('we', 173), ('session', 174), ('speak', 175), ('recommend', 176), ('abused', 177), ('said', 178), ('talking', 179), ('much', 180), ('built', 181), ('let', 182), ('they', 183), ('many', 184), ('year', 185), ('here', 186), ('life', 187), ('all', 188), (\"can't\", 189), (\"who's\", 190), ('why', 191), ('learn', 192), ('smart', 193), ('cute', 194), ('web', 195), ('based', 196), ('training', 197), ('details', 198), ('sessions', 199), ('list', 200), ('sick', 201), ('workplace', 202), ('complaint', 203), ('proceed', 204), ('morning', 205), ('again', 206), ('friend', 207), ('an', 208), ('idea', 209), ('make', 210), ('see', 211), ('go', 212), ('later', 213), ('nice', 214), ('then', 215), ('years', 216), ('if', 217), (\"where's\", 218), ('guess', 219), (\"that's\", 220), ('conversation', 221), ('owner', 222), ('invented', 223), ('should', 224), ('could', 225), ('anything', 226), ('find', 227), ('gorgeous', 228), ('date', 229), ('would', 230), ('skip', 231), ('ice', 232), ('cream', 233), ('change', 234), ('rasa', 235), ('career', 236), ('searching', 237), ('harassed', 238), ('verbally', 239), ('raise', 240), ('harassment', 241), ('contact', 242), ('hr', 243), ('tcs', 244), ('robot', 245), (\"let's\", 246), ('feeling', 247), ('sad', 248), ('amazing', 249), ('exactly', 250), ('process', 251), ('constructed', 252), ('devised', 253), ('into', 254), ('meet', 255), ('next', 256), ('long', 257), ('live', 258), ('grow', 259), ('town', 260), ('residence', 261), ('things', 262), ('right', 263), ('man', 264), ('nothing', 265), ('call', 266), ('yourself', 267), ('ask', 268), ('had', 269), ('use', 270), ('something', 271), ('one', 272), ('annoy', 273), ('better', 274), ('attractive', 275), ('awesome', 276), ('sorry', 277), ('oh', 278), ('dripping', 279), ('care', 280), ('already', 281), ('told', 282), ('days', 283), ('docs', 284), ('immediately', 285), ('employee', 286), ('different', 287), ('take', 288), ('topic', 289), ('experties', 290), ('unable', 291), ('manager', 292), ('discriminated', 293), ('sweet', 294), ('well', 295), ('evening', 296), ('extremely', 297), ('human', 298), ('perfect', 299), ('bunch', 300), ('lot', 301), ('news', 302), (\"i'd\", 303), ('formed', 304), ('put', 305), ('create', 306), ('used', 307), ('conceived', 308), ('through', 309), ('ya', 310), ('udo', 311), ('b', 312), ('celebrated', 313), ('around', 314), ('alive', 315), ('country', 316), ('claim', 317), ('location', 318), ('house', 319), ('having', 320), ('okay', 321), ('feel', 322), ('treating', 323), ('cant', 324), ('shit', 325), ('helpful', 326), ('designed', 327), ('generated', 328), ('creator', 329), ('owns', 330), ('thought', 331), ('mind', 332), ('working', 333), ('possible', 334), ('knock', 335), ('incredibly', 336), ('too', 337), ('questions', 338), ('clever', 339), ('trained', 340), ('handsome', 341), ('wonderful', 342), ('free', 343), ('joke', 344), ('back', 345), ('yep', 346), ('single', 347), ('food', 348), ('bots', 349), ('wait', 350), ('chicken', 351), ('french', 352), ('restaurant', 353), ('email', 354), (\"doesn't\", 355), ('hate', 356), ('phone', 357), ('shitmuncher', 358), ('mouse', 359), ('couple', 360), ('newsletter', 361), ('than', 362), ('accept', 363), ('nah', 364), ('those', 365), ('giving', 366), ('eloborate', 367), ('gist', 368), ('path', 369), ('disscuss', 370), ('most', 371), ('flexi', 372), ('environment', 373), ('quitting', 374), ('file', 375), ('team', 376), ('folks', 377), ('mister', 378), ('pal', 379), ('everybody', 380), ('anybody', 381), ('boy', 382), ('sweatheart', 383), ('ayyyy', 384), ('whaddup', 385), ('helllllloooooo', 386), ('dude', 387), ('afternoon', 388), ('cheers', 389), ('bro', 390), ('ton', 391), ('making', 392), ('specify', 393), ('together', 394), ('say', 395), ('manner', 396), ('shaped', 397), ('originated', 398), ('set', 399), ('method', 400), ('fashioned', 401), ('night', 402), ('gotta', 403), ('catch', 404), ('k', 405), ('byyye', 406), ('cu', 407), ('exact', 408), ('birthdate', 409), ('celebrate', 410), ('candles', 411), ('cake', 412), ('friends', 413), ('same', 414), ('month', 415), ('lived', 416), ('occupied', 417), ('earth', 418), ('state', 419), ('before', 420), ('parents', 421), ('spend', 422), ('youth', 423), ('consider', 424), ('citizenship', 425), ('lay', 426), ('heritage', 427), ('birthplace', 428), ('root', 429), ('origin', 430), ('originate', 431), ('roots', 432), ('origins', 433), ('area', 434), ('place', 435), ('came', 436), ('ahoy', 437), ('matey', 438), ('alright', 439), ('hanging', 440), (\"how've\", 441), ('fine', 442), ('helping', 443), ('doesnt', 444), ('hm', 445), ('share', 446), ('may', 447), ('creators', 448), ('founders', 449), ('brought', 450), ('existence', 451), ('builder', 452), ('fabricated', 453), ('generate', 454), ('behind', 455), ('legal', 456), ('programmer', 457), ('modeled', 458), ('produced', 459), ('went', 460), ('trouble', 461), ('setting', 462), ('yoi', 463), ('stuck', 464), ('able', 465), ('asked', 466), ('options', 467), ('sentence', 468), ('speaking', 469), ('r', 470), ('ur', 471), ('called', 472), ('introduction', 473), ('introduce', 474), ('intro', 475), ('irritating', 476), ('such', 477), ('starting', 478), ('answering', 479), ('study', 480), ('must', 481), ('smarter', 482), ('qualified', 483), ('useful', 484), ('out', 485), ('box', 486), ('smarty', 487), ('pants', 488), ('improve', 489), ('brains', 490), ('fantastic', 491), ('cutie', 492), (\"when's\", 493), ('busy', 494), ('might', 495), ('dying', 496), ('hunger', 497), ('interesting', 498), ('everyone', 499), ('because', 500), ('im', 501), ('answers', 502), ('terribly', 503), ('speech', 504), ('whole', 505), ('thing', 506), ('end', 507), ('disregard', 508), ('require', 509), ('sure', 510), ('barbara', 511), ('still', 512), ('married', 513), ('singe', 514), ('mingle', 515), ('pizza', 516), ('someone', 517), ('police', 518), ('died', 519), ('loser', 520), ('lmao', 521), ('mad', 522), ('guy', 523), ('crazy', 524), ('hang', 525), ('changed', 526), ('stupid', 527), ('keep', 528), ('chatting', 529), ('god', 530), ('our', 531), ('common', 532), ('try', 533), ('suck', 534), ('dont', 535), ('instruct', 536), ('ohh', 537), ('confused', 538), ('weather', 539), ('cone', 540), ('catches', 541), ('drink', 542), ('non', 543), ('picture', 544), ('cuisine', 545), ('shown', 546), ('gluten', 547), ('gave', 548), ('address', 549), ('break', 550), ('wanna', 551), ('link', 552), ('frustrating', 553), ('experience', 554), ('silly', 555), ('first', 556), ('wasteland', 557), ('full', 558), ('broken', 559), ('parts', 560), ('deal', 561), ('anser', 562), ('noises', 563), ('wife', 564), ('weekend', 565), ('kids', 566), ('rather', 567), ('dull', 568), ('personal', 569), ('or', 570), ('using', 571), ('core', 572), ('nlu', 573), ('engineers', 574), ('favourite', 575), ('tamed', 576), ('arrive', 577), ('doorstep', 578), ('red', 579), ('rose', 580), ('carries', 581), ('between', 582), ('its', 583), ('teeth', 584), ('tame', 585), (\"isn't\", 586), ('spam', 587), ('cannot', 588), ('rephrase', 589), ('rest', 590), ('api', 591), ('badass', 592), ('business', 593), ('implementing', 594), ('coolest', 595), ('imagine', 596), ('contacted', 597), ('ago', 598), (\"didn't\", 599), ('response', 600), ('hurry', 601), ('deadline', 602), ('two', 603), ('weeks', 604), ('deliver', 605), ('big', 606), ('kannst', 607), ('du', 608), ('auch', 609), ('deutsch', 610), ('worth', 611), ('subscription', 612), ('pity', 613), ('number', 614), (\"akela's\", 615), ('cats', 616), ('don’t', 617), ('english', 618), ('site', 619), ('definitely', 620), ('absolutely', 621), ('afraid', 622), ('sir', 623), (\"ma'am\", 624), ('decline', 625), ('wrong', 626), ('either', 627), ('neither', 628), ('these', 629), ('id', 630), ('empid', 631), (\"haven't\", 632), ('decided', 633), ('yet', 634), ('popukar', 635), ('competency', 636), ('various', 637), ('carrier', 638), ('guidence', 639), ('skilled', 640), ('technology', 641), ('upcoming', 642), ('after', 643), ('status', 644), ('doubt', 645), ('gimme', 646), ('anyone', 647), ('touch', 648), ('suggested', 649), ('expertise', 650), ('highest', 651), ('ratings', 652), ('recommendation', 653), ('trending', 654), ('market', 655), ('vlit', 656), ('virtual', 657), ('instructor', 658), ('lead', 659), ('recently', 660), ('pursue', 661), ('higher', 662), ('studies', 663), ('serious', 664), ('health', 665), ('issues', 666), ('balance', 667), ('pregnant', 668), ('stressed', 669), ('lately', 670), ('culture', 671), ('longer', 672), ('continue', 673), ('organisation', 674), ('shifts', 675), ('cope', 676), ('shift', 677), ('timings', 678), ('far', 679), ('away', 680), ('tl', 681), ('collegue', 682), ('abusing', 683), ('incident', 684), ('facing', 685), ('sexual', 686), ('physically', 687), ('toxic', 688), ('due', 689), ('gender', 690), ('belong', 691), ('lgbt', 692), ('community', 693), ('caste', 694), ('bus', 695), ('driver', 696), ('issue', 697), ('thinking', 698), ('gym', 699), ('trainer', 700), ('harassing', 701), ('constantly', 702), ('mates', 703), ('complain', 704), ('mate', 705), ('assaulted', 706), ('religion', 707), ('underpaid', 708), ('feels', 709), ('pointless', 710), ('wish', 711), ('direction', 712), ('wanting', 713), ('has', 714), ('kind', 715), ('delete', 716), ('3', 717)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXW68dfNtTNs"
      },
      "source": [
        "with open('/content/drive/My Drive/WordPrediction/tokenizer_bot.pickle', 'wb') as tkn_bot:\n",
        "    pickle.dump(tokenizer, tkn_bot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt-i522Dtg6H"
      },
      "source": [
        "tokenizer = pickle.load(open(\"/content/drive/My Drive/WordPrediction/tokenizer_bot.pickle\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HedGjWBztw3y",
        "outputId": "17fac091-7997-494c-dd9d-42d5c3df110f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras_preprocessing.text.Tokenizer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5MHS0lPt0AH",
        "outputId": "1a6870ad-fe5c-4d5f-a304-a1b02ed45cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer.word_index.items()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('you', 1), ('what', 2), ('me', 3), ('i', 4), ('can', 5), ('courses', 6), ('are', 7), ('tell', 8), ('about', 9), ('is', 10), ('to', 11), ('course', 12), ('the', 13), ('lia', 14), ('do', 15), ('your', 16), ('how', 17), ('of', 18), ('am', 19), ('want', 20), ('competencies', 21), ('explain', 22), ('know', 23), ('a', 24), ('popular', 25), ('by', 26), ('means', 27), ('mean', 28), ('on', 29), ('who', 30), ('new', 31), ('were', 32), ('my', 33), ('more', 34), ('meaning', 35), ('not', 36), ('elaborate', 37), (\"what's\", 38), ('cancel', 39), ('sme', 40), ('it', 41), ('for', 42), ('help', 43), ('delivery', 44), ('modes', 45), ('where', 46), ('ilt', 47), ('feedback', 48), ('so', 49), ('expired', 50), (\"you're\", 51), ('mandatory', 52), ('page', 53), ('search', 54), ('learning', 55), ('points', 56), ('have', 57), ('please', 58), ('bot', 59), ('this', 60), ('prerequisites', 61), ('reviews', 62), ('notifications', 63), ('in', 64), ('from', 65), ('launch', 66), ('drop', 67), ('good', 68), ('that', 69), ('unaware', 70), ('aware', 71), ('enroll', 72), ('leave', 73), ('activity', 74), ('any', 75), ('overview', 76), ('quit', 77), ('with', 78), ('look', 79), ('very', 80), ('no', 81), ('some', 82), ('wbt', 83), (\"don't\", 84), ('now', 85), ('answer', 86), ('thanks', 87), ('give', 88), ('be', 89), ('get', 90), ('annoying', 91), ('made', 92), ('did', 93), ('bye', 94), ('really', 95), ('work', 96), ('up', 97), ('which', 98), ('age', 99), ('will', 100), ('job', 101), ('apply', 102), ('being', 103), ('just', 104), ('beautiful', 105), ('boring', 106), ('leaves', 107), ('hi', 108), (\"i'm\", 109), ('and', 110), ('need', 111), ('vilt', 112), ('show', 113), ('suggest', 114), ('offer', 115), ('hey', 116), ('there', 117), ('like', 118), ('old', 119), ('birthday', 120), ('today', 121), ('hungry', 122), ('request', 123), ('hello', 124), ('created', 125), ('was', 126), ('day', 127), ('company', 128), ('does', 129), ('pretty', 130), ('types', 131), ('talk', 132), ('great', 133), ('thank', 134), ('ok', 135), ('come', 136), ('born', 137), ('birth', 138), ('think', 139), ('else', 140), ('boss', 141), ('question', 142), ('looking', 143), ('forget', 144), ('type', 145), ('whats', 146), ('when', 147), ('city', 148), ('home', 149), ('going', 150), ('doing', 151), ('stop', 152), ('information', 153), ('marry', 154), ('added', 155), (\"it's\", 156), ('bad', 157), ('everything', 158), ('cool', 159), ('as', 160), ('way', 161), ('build', 162), ('u', 163), ('last', 164), ('been', 165), ('at', 166), ('name', 167), ('hometown', 168), (\"how's\", 169), ('but', 170), ('eat', 171), ('bored', 172), ('we', 173), ('session', 174), ('speak', 175), ('recommend', 176), ('abused', 177), ('said', 178), ('talking', 179), ('much', 180), ('built', 181), ('let', 182), ('they', 183), ('many', 184), ('year', 185), ('here', 186), ('life', 187), ('all', 188), (\"can't\", 189), (\"who's\", 190), ('why', 191), ('learn', 192), ('smart', 193), ('cute', 194), ('web', 195), ('based', 196), ('training', 197), ('details', 198), ('sessions', 199), ('list', 200), ('sick', 201), ('workplace', 202), ('complaint', 203), ('proceed', 204), ('morning', 205), ('again', 206), ('friend', 207), ('an', 208), ('idea', 209), ('make', 210), ('see', 211), ('go', 212), ('later', 213), ('nice', 214), ('then', 215), ('years', 216), ('if', 217), (\"where's\", 218), ('guess', 219), (\"that's\", 220), ('conversation', 221), ('owner', 222), ('invented', 223), ('should', 224), ('could', 225), ('anything', 226), ('find', 227), ('gorgeous', 228), ('date', 229), ('would', 230), ('skip', 231), ('ice', 232), ('cream', 233), ('change', 234), ('rasa', 235), ('career', 236), ('searching', 237), ('harassed', 238), ('verbally', 239), ('raise', 240), ('harassment', 241), ('contact', 242), ('hr', 243), ('tcs', 244), ('robot', 245), (\"let's\", 246), ('feeling', 247), ('sad', 248), ('amazing', 249), ('exactly', 250), ('process', 251), ('constructed', 252), ('devised', 253), ('into', 254), ('meet', 255), ('next', 256), ('long', 257), ('live', 258), ('grow', 259), ('town', 260), ('residence', 261), ('things', 262), ('right', 263), ('man', 264), ('nothing', 265), ('call', 266), ('yourself', 267), ('ask', 268), ('had', 269), ('use', 270), ('something', 271), ('one', 272), ('annoy', 273), ('better', 274), ('attractive', 275), ('awesome', 276), ('sorry', 277), ('oh', 278), ('dripping', 279), ('care', 280), ('already', 281), ('told', 282), ('days', 283), ('docs', 284), ('immediately', 285), ('employee', 286), ('different', 287), ('take', 288), ('topic', 289), ('experties', 290), ('unable', 291), ('manager', 292), ('discriminated', 293), ('sweet', 294), ('well', 295), ('evening', 296), ('extremely', 297), ('human', 298), ('perfect', 299), ('bunch', 300), ('lot', 301), ('news', 302), (\"i'd\", 303), ('formed', 304), ('put', 305), ('create', 306), ('used', 307), ('conceived', 308), ('through', 309), ('ya', 310), ('udo', 311), ('b', 312), ('celebrated', 313), ('around', 314), ('alive', 315), ('country', 316), ('claim', 317), ('location', 318), ('house', 319), ('having', 320), ('okay', 321), ('feel', 322), ('treating', 323), ('cant', 324), ('shit', 325), ('helpful', 326), ('designed', 327), ('generated', 328), ('creator', 329), ('owns', 330), ('thought', 331), ('mind', 332), ('working', 333), ('possible', 334), ('knock', 335), ('incredibly', 336), ('too', 337), ('questions', 338), ('clever', 339), ('trained', 340), ('handsome', 341), ('wonderful', 342), ('free', 343), ('joke', 344), ('back', 345), ('yep', 346), ('single', 347), ('food', 348), ('bots', 349), ('wait', 350), ('chicken', 351), ('french', 352), ('restaurant', 353), ('email', 354), (\"doesn't\", 355), ('hate', 356), ('phone', 357), ('shitmuncher', 358), ('mouse', 359), ('couple', 360), ('newsletter', 361), ('than', 362), ('accept', 363), ('nah', 364), ('those', 365), ('giving', 366), ('eloborate', 367), ('gist', 368), ('path', 369), ('disscuss', 370), ('most', 371), ('flexi', 372), ('environment', 373), ('quitting', 374), ('file', 375), ('team', 376), ('folks', 377), ('mister', 378), ('pal', 379), ('everybody', 380), ('anybody', 381), ('boy', 382), ('sweatheart', 383), ('ayyyy', 384), ('whaddup', 385), ('helllllloooooo', 386), ('dude', 387), ('afternoon', 388), ('cheers', 389), ('bro', 390), ('ton', 391), ('making', 392), ('specify', 393), ('together', 394), ('say', 395), ('manner', 396), ('shaped', 397), ('originated', 398), ('set', 399), ('method', 400), ('fashioned', 401), ('night', 402), ('gotta', 403), ('catch', 404), ('k', 405), ('byyye', 406), ('cu', 407), ('exact', 408), ('birthdate', 409), ('celebrate', 410), ('candles', 411), ('cake', 412), ('friends', 413), ('same', 414), ('month', 415), ('lived', 416), ('occupied', 417), ('earth', 418), ('state', 419), ('before', 420), ('parents', 421), ('spend', 422), ('youth', 423), ('consider', 424), ('citizenship', 425), ('lay', 426), ('heritage', 427), ('birthplace', 428), ('root', 429), ('origin', 430), ('originate', 431), ('roots', 432), ('origins', 433), ('area', 434), ('place', 435), ('came', 436), ('ahoy', 437), ('matey', 438), ('alright', 439), ('hanging', 440), (\"how've\", 441), ('fine', 442), ('helping', 443), ('doesnt', 444), ('hm', 445), ('share', 446), ('may', 447), ('creators', 448), ('founders', 449), ('brought', 450), ('existence', 451), ('builder', 452), ('fabricated', 453), ('generate', 454), ('behind', 455), ('legal', 456), ('programmer', 457), ('modeled', 458), ('produced', 459), ('went', 460), ('trouble', 461), ('setting', 462), ('yoi', 463), ('stuck', 464), ('able', 465), ('asked', 466), ('options', 467), ('sentence', 468), ('speaking', 469), ('r', 470), ('ur', 471), ('called', 472), ('introduction', 473), ('introduce', 474), ('intro', 475), ('irritating', 476), ('such', 477), ('starting', 478), ('answering', 479), ('study', 480), ('must', 481), ('smarter', 482), ('qualified', 483), ('useful', 484), ('out', 485), ('box', 486), ('smarty', 487), ('pants', 488), ('improve', 489), ('brains', 490), ('fantastic', 491), ('cutie', 492), (\"when's\", 493), ('busy', 494), ('might', 495), ('dying', 496), ('hunger', 497), ('interesting', 498), ('everyone', 499), ('because', 500), ('im', 501), ('answers', 502), ('terribly', 503), ('speech', 504), ('whole', 505), ('thing', 506), ('end', 507), ('disregard', 508), ('require', 509), ('sure', 510), ('barbara', 511), ('still', 512), ('married', 513), ('singe', 514), ('mingle', 515), ('pizza', 516), ('someone', 517), ('police', 518), ('died', 519), ('loser', 520), ('lmao', 521), ('mad', 522), ('guy', 523), ('crazy', 524), ('hang', 525), ('changed', 526), ('stupid', 527), ('keep', 528), ('chatting', 529), ('god', 530), ('our', 531), ('common', 532), ('try', 533), ('suck', 534), ('dont', 535), ('instruct', 536), ('ohh', 537), ('confused', 538), ('weather', 539), ('cone', 540), ('catches', 541), ('drink', 542), ('non', 543), ('picture', 544), ('cuisine', 545), ('shown', 546), ('gluten', 547), ('gave', 548), ('address', 549), ('break', 550), ('wanna', 551), ('link', 552), ('frustrating', 553), ('experience', 554), ('silly', 555), ('first', 556), ('wasteland', 557), ('full', 558), ('broken', 559), ('parts', 560), ('deal', 561), ('anser', 562), ('noises', 563), ('wife', 564), ('weekend', 565), ('kids', 566), ('rather', 567), ('dull', 568), ('personal', 569), ('or', 570), ('using', 571), ('core', 572), ('nlu', 573), ('engineers', 574), ('favourite', 575), ('tamed', 576), ('arrive', 577), ('doorstep', 578), ('red', 579), ('rose', 580), ('carries', 581), ('between', 582), ('its', 583), ('teeth', 584), ('tame', 585), (\"isn't\", 586), ('spam', 587), ('cannot', 588), ('rephrase', 589), ('rest', 590), ('api', 591), ('badass', 592), ('business', 593), ('implementing', 594), ('coolest', 595), ('imagine', 596), ('contacted', 597), ('ago', 598), (\"didn't\", 599), ('response', 600), ('hurry', 601), ('deadline', 602), ('two', 603), ('weeks', 604), ('deliver', 605), ('big', 606), ('kannst', 607), ('du', 608), ('auch', 609), ('deutsch', 610), ('worth', 611), ('subscription', 612), ('pity', 613), ('number', 614), (\"akela's\", 615), ('cats', 616), ('don’t', 617), ('english', 618), ('site', 619), ('definitely', 620), ('absolutely', 621), ('afraid', 622), ('sir', 623), (\"ma'am\", 624), ('decline', 625), ('wrong', 626), ('either', 627), ('neither', 628), ('these', 629), ('id', 630), ('empid', 631), (\"haven't\", 632), ('decided', 633), ('yet', 634), ('popukar', 635), ('competency', 636), ('various', 637), ('carrier', 638), ('guidence', 639), ('skilled', 640), ('technology', 641), ('upcoming', 642), ('after', 643), ('status', 644), ('doubt', 645), ('gimme', 646), ('anyone', 647), ('touch', 648), ('suggested', 649), ('expertise', 650), ('highest', 651), ('ratings', 652), ('recommendation', 653), ('trending', 654), ('market', 655), ('vlit', 656), ('virtual', 657), ('instructor', 658), ('lead', 659), ('recently', 660), ('pursue', 661), ('higher', 662), ('studies', 663), ('serious', 664), ('health', 665), ('issues', 666), ('balance', 667), ('pregnant', 668), ('stressed', 669), ('lately', 670), ('culture', 671), ('longer', 672), ('continue', 673), ('organisation', 674), ('shifts', 675), ('cope', 676), ('shift', 677), ('timings', 678), ('far', 679), ('away', 680), ('tl', 681), ('collegue', 682), ('abusing', 683), ('incident', 684), ('facing', 685), ('sexual', 686), ('physically', 687), ('toxic', 688), ('due', 689), ('gender', 690), ('belong', 691), ('lgbt', 692), ('community', 693), ('caste', 694), ('bus', 695), ('driver', 696), ('issue', 697), ('thinking', 698), ('gym', 699), ('trainer', 700), ('harassing', 701), ('constantly', 702), ('mates', 703), ('complain', 704), ('mate', 705), ('assaulted', 706), ('religion', 707), ('underpaid', 708), ('feels', 709), ('pointless', 710), ('wish', 711), ('direction', 712), ('wanting', 713), ('has', 714), ('kind', 715), ('delete', 716), ('3', 717)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-hMkRpt2wW"
      },
      "source": [
        "with open('/content/drive/My Drive/WordPrediction/embedding_matrix_bot.pickle', 'wb') as emb_mat:\n",
        "    pickle.dump(embedding_matrix, emb_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAMgmJMLuGc4"
      },
      "source": [
        "embedding_matrix = pickle.load(open(\"/content/drive/My Drive/WordPrediction/embedding_matrix_bot.pickle\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OiTM6aHuPOI"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzzip2uwuaXt"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NMqM5z3udBx",
        "outputId": "95052b9d-26df-45ee-cc4c-3a597286f656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100,weights=[embedding_matrix], input_length=max_sequence_len-1,trainable = False))\n",
        "model.add(LSTM(150, return_sequences = True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "model.fit(predictors, label, epochs=200, verbose=1, callbacks=[earlystop])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/200\n",
            "6166/6166 [==============================] - 13s 2ms/step - loss: 5.3272 - acc: 0.0795\n",
            "Epoch 2/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 5.0732 - acc: 0.0886\n",
            "Epoch 3/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 5.0179 - acc: 0.0924\n",
            "Epoch 4/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.7644 - acc: 0.1364\n",
            "Epoch 5/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.3703 - acc: 0.2000\n",
            "Epoch 6/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.0713 - acc: 0.2400\n",
            "Epoch 7/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.8417 - acc: 0.2738\n",
            "Epoch 8/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.6458 - acc: 0.2957\n",
            "Epoch 9/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.4789 - acc: 0.3159\n",
            "Epoch 10/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.3268 - acc: 0.3341\n",
            "Epoch 11/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.1972 - acc: 0.3459\n",
            "Epoch 12/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.0768 - acc: 0.3612\n",
            "Epoch 13/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.9769 - acc: 0.3694\n",
            "Epoch 14/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.8767 - acc: 0.3827\n",
            "Epoch 15/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.7950 - acc: 0.3858\n",
            "Epoch 16/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.7131 - acc: 0.3964\n",
            "Epoch 17/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.6397 - acc: 0.4056\n",
            "Epoch 18/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.5746 - acc: 0.4149\n",
            "Epoch 19/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.5079 - acc: 0.4194\n",
            "Epoch 20/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.4490 - acc: 0.4269\n",
            "Epoch 21/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.3881 - acc: 0.4363\n",
            "Epoch 22/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.3393 - acc: 0.4424\n",
            "Epoch 23/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.2822 - acc: 0.4526\n",
            "Epoch 24/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.2330 - acc: 0.4578\n",
            "Epoch 25/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.2045 - acc: 0.4632\n",
            "Epoch 26/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.1548 - acc: 0.4692\n",
            "Epoch 27/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.1088 - acc: 0.4786\n",
            "Epoch 28/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.0692 - acc: 0.4784\n",
            "Epoch 29/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.0326 - acc: 0.4869\n",
            "Epoch 30/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.9938 - acc: 0.4904\n",
            "Epoch 31/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.9641 - acc: 0.4953\n",
            "Epoch 32/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.9239 - acc: 0.5015\n",
            "Epoch 33/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8914 - acc: 0.5070\n",
            "Epoch 34/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8582 - acc: 0.5139\n",
            "Epoch 35/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8263 - acc: 0.5208\n",
            "Epoch 36/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.7994 - acc: 0.5217\n",
            "Epoch 37/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.7706 - acc: 0.5279\n",
            "Epoch 38/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.7609 - acc: 0.5300\n",
            "Epoch 39/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.7442 - acc: 0.5354\n",
            "Epoch 40/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.7091 - acc: 0.5407\n",
            "Epoch 41/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.6867 - acc: 0.5440\n",
            "Epoch 42/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.6614 - acc: 0.5490\n",
            "Epoch 43/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.6430 - acc: 0.5500\n",
            "Epoch 44/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.6212 - acc: 0.5610\n",
            "Epoch 45/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.6014 - acc: 0.5558\n",
            "Epoch 46/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.5786 - acc: 0.5645\n",
            "Epoch 47/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5582 - acc: 0.5733\n",
            "Epoch 48/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5431 - acc: 0.5694\n",
            "Epoch 49/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5250 - acc: 0.5774\n",
            "Epoch 50/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.5031 - acc: 0.5804\n",
            "Epoch 51/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.4874 - acc: 0.5806\n",
            "Epoch 52/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.4692 - acc: 0.5933\n",
            "Epoch 53/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4572 - acc: 0.5892\n",
            "Epoch 54/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4390 - acc: 0.5928\n",
            "Epoch 55/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4238 - acc: 0.5958\n",
            "Epoch 56/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.4097 - acc: 0.5986\n",
            "Epoch 57/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3902 - acc: 0.6048\n",
            "Epoch 58/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3824 - acc: 0.6070\n",
            "Epoch 59/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3660 - acc: 0.6087\n",
            "Epoch 60/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3590 - acc: 0.6061\n",
            "Epoch 61/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3418 - acc: 0.6127\n",
            "Epoch 62/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3345 - acc: 0.6111\n",
            "Epoch 63/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3219 - acc: 0.6153\n",
            "Epoch 64/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.3095 - acc: 0.6169\n",
            "Epoch 65/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2982 - acc: 0.6281\n",
            "Epoch 66/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2874 - acc: 0.6257\n",
            "Epoch 67/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2796 - acc: 0.6215\n",
            "Epoch 68/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2695 - acc: 0.6260\n",
            "Epoch 69/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2590 - acc: 0.6271\n",
            "Epoch 70/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2487 - acc: 0.6315\n",
            "Epoch 71/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2366 - acc: 0.6314\n",
            "Epoch 72/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2315 - acc: 0.6361\n",
            "Epoch 73/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2224 - acc: 0.6356\n",
            "Epoch 74/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2143 - acc: 0.6349\n",
            "Epoch 75/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.2063 - acc: 0.6392\n",
            "Epoch 76/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1964 - acc: 0.6395\n",
            "Epoch 77/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1901 - acc: 0.6380\n",
            "Epoch 78/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1904 - acc: 0.6414\n",
            "Epoch 79/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1779 - acc: 0.6440\n",
            "Epoch 80/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1678 - acc: 0.6476\n",
            "Epoch 81/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1592 - acc: 0.6500\n",
            "Epoch 82/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1541 - acc: 0.6466\n",
            "Epoch 83/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1484 - acc: 0.6487\n",
            "Epoch 84/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1435 - acc: 0.6469\n",
            "Epoch 85/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1388 - acc: 0.6507\n",
            "Epoch 86/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1318 - acc: 0.6518\n",
            "Epoch 87/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1281 - acc: 0.6539\n",
            "Epoch 88/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1232 - acc: 0.6516\n",
            "Epoch 89/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1146 - acc: 0.6552\n",
            "Epoch 90/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1093 - acc: 0.6546\n",
            "Epoch 91/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1062 - acc: 0.6584\n",
            "Epoch 92/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1004 - acc: 0.6568\n",
            "Epoch 93/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0908 - acc: 0.6555\n",
            "Epoch 94/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0929 - acc: 0.6562\n",
            "Epoch 95/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0871 - acc: 0.6586\n",
            "Epoch 96/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0835 - acc: 0.6584\n",
            "Epoch 97/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0768 - acc: 0.6622\n",
            "Epoch 98/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0756 - acc: 0.6606\n",
            "Epoch 99/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0697 - acc: 0.6619\n",
            "Epoch 100/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0681 - acc: 0.6580\n",
            "Epoch 101/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0619 - acc: 0.6601\n",
            "Epoch 102/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0581 - acc: 0.6628\n",
            "Epoch 103/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0560 - acc: 0.6596\n",
            "Epoch 104/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0537 - acc: 0.6638\n",
            "Epoch 105/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0500 - acc: 0.6599\n",
            "Epoch 106/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0494 - acc: 0.6596\n",
            "Epoch 107/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0475 - acc: 0.6614\n",
            "Epoch 108/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0447 - acc: 0.6659\n",
            "Epoch 109/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0387 - acc: 0.6612\n",
            "Epoch 110/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0365 - acc: 0.6625\n",
            "Epoch 111/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.0345 - acc: 0.6632\n",
            "Epoch 112/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0338 - acc: 0.6623\n",
            "Epoch 113/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0244 - acc: 0.6625\n",
            "Epoch 114/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0257 - acc: 0.6641\n",
            "Epoch 115/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0230 - acc: 0.6625\n",
            "Epoch 116/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0233 - acc: 0.6632\n",
            "Epoch 117/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0245 - acc: 0.6617\n",
            "Epoch 118/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0170 - acc: 0.6653\n",
            "Epoch 119/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0168 - acc: 0.6662\n",
            "Epoch 120/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0158 - acc: 0.6679\n",
            "Epoch 121/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0126 - acc: 0.6687\n",
            "Epoch 122/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0108 - acc: 0.6672\n",
            "Epoch 123/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0108 - acc: 0.6625\n",
            "Epoch 124/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0074 - acc: 0.6654\n",
            "Epoch 125/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0027 - acc: 0.6679\n",
            "Epoch 126/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0026 - acc: 0.6687\n",
            "Epoch 127/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0031 - acc: 0.6683\n",
            "Epoch 128/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9977 - acc: 0.6675\n",
            "Epoch 129/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9976 - acc: 0.6672\n",
            "Epoch 130/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9958 - acc: 0.6682\n",
            "Epoch 131/200\n",
            "6166/6166 [==============================] - 13s 2ms/step - loss: 0.9969 - acc: 0.6651\n",
            "Epoch 132/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9944 - acc: 0.6672\n",
            "Epoch 133/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9927 - acc: 0.6667\n",
            "Epoch 134/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9933 - acc: 0.6683\n",
            "Epoch 135/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9907 - acc: 0.6670\n",
            "Epoch 136/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9897 - acc: 0.6662\n",
            "Epoch 137/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9895 - acc: 0.6677\n",
            "Epoch 138/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9910 - acc: 0.6628\n",
            "Epoch 139/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9838 - acc: 0.6700\n",
            "Epoch 140/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9865 - acc: 0.6664\n",
            "Epoch 141/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9842 - acc: 0.6612\n",
            "Epoch 142/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9873 - acc: 0.6627\n",
            "Epoch 143/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9808 - acc: 0.6620\n",
            "Epoch 144/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9809 - acc: 0.6661\n",
            "Epoch 145/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9795 - acc: 0.6666\n",
            "Epoch 146/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9847 - acc: 0.6664\n",
            "Epoch 147/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9769 - acc: 0.6675\n",
            "Epoch 148/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9804 - acc: 0.6632\n",
            "Epoch 149/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9795 - acc: 0.6640\n",
            "Epoch 150/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9748 - acc: 0.6682\n",
            "Epoch 151/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9773 - acc: 0.6685\n",
            "Epoch 152/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9740 - acc: 0.6687\n",
            "Epoch 153/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9751 - acc: 0.6633\n",
            "Epoch 154/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9723 - acc: 0.6690\n",
            "Epoch 155/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9722 - acc: 0.6653\n",
            "Epoch 156/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9698 - acc: 0.6698\n",
            "Epoch 157/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9720 - acc: 0.6662\n",
            "Epoch 158/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9719 - acc: 0.6683\n",
            "Epoch 159/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9712 - acc: 0.6640\n",
            "Epoch 160/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9689 - acc: 0.6674\n",
            "Epoch 161/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9713 - acc: 0.6662\n",
            "Epoch 162/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9755 - acc: 0.6649\n",
            "Epoch 163/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9671 - acc: 0.6693\n",
            "Epoch 164/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9679 - acc: 0.6623\n",
            "Epoch 165/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9645 - acc: 0.6636\n",
            "Epoch 166/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9662 - acc: 0.6679\n",
            "Epoch 167/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9656 - acc: 0.6662\n",
            "Epoch 168/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9639 - acc: 0.6685\n",
            "Epoch 169/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9641 - acc: 0.6677\n",
            "Epoch 170/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9623 - acc: 0.6654\n",
            "Epoch 171/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9644 - acc: 0.6662\n",
            "Epoch 172/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9659 - acc: 0.6670\n",
            "Epoch 173/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9620 - acc: 0.6680\n",
            "Epoch 174/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9620 - acc: 0.6682\n",
            "Epoch 175/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9631 - acc: 0.6656\n",
            "Epoch 176/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9627 - acc: 0.6683\n",
            "Epoch 177/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9595 - acc: 0.6670\n",
            "Epoch 178/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9572 - acc: 0.6679\n",
            "Epoch 179/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9623 - acc: 0.6651\n",
            "Epoch 180/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9571 - acc: 0.6674\n",
            "Epoch 181/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9565 - acc: 0.6661\n",
            "Epoch 182/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9584 - acc: 0.6703\n",
            "Epoch 183/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9587 - acc: 0.6659\n",
            "Epoch 184/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9612 - acc: 0.6708\n",
            "Epoch 185/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9649 - acc: 0.6672\n",
            "Epoch 186/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9583 - acc: 0.6664\n",
            "Epoch 187/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9533 - acc: 0.6688\n",
            "Epoch 188/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9522 - acc: 0.6696\n",
            "Epoch 189/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9532 - acc: 0.6675\n",
            "Epoch 190/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9537 - acc: 0.6677\n",
            "Epoch 191/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9529 - acc: 0.6675\n",
            "Epoch 192/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9529 - acc: 0.6670\n",
            "Epoch 193/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9549 - acc: 0.6666\n",
            "Epoch 194/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9517 - acc: 0.6703\n",
            "Epoch 195/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9511 - acc: 0.6687\n",
            "Epoch 196/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9506 - acc: 0.6643\n",
            "Epoch 197/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9537 - acc: 0.6672\n",
            "Epoch 198/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9508 - acc: 0.6682\n",
            "Epoch 199/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9501 - acc: 0.6659\n",
            "Epoch 200/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9511 - acc: 0.6685\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 18, 100)           71800     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 18, 150)           150600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               100400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 718)               72518     \n",
            "=================================================================\n",
            "Total params: 395,318\n",
            "Trainable params: 323,518\n",
            "Non-trainable params: 71,800\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdFI-TiwujUZ"
      },
      "source": [
        "def generate_top3_text_model(seed_text, max_sequence_len=19):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    flattened_predict = predicted.flatten()\n",
        "    pred_idx_list = flattened_predict.argsort()[:-4:-1]\n",
        "    output_word = [seed_text]\n",
        "    for i in pred_idx_list:\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == i:\n",
        "                output_word.append(word)\n",
        "                break\n",
        "    return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6Voh0Q8QPD"
      },
      "source": [
        "\n",
        "\n",
        "model.save('/content/drive/My Drive/WordPrediction/modelbot_200.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "del model  # deletes the existing model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG4vI6Bq8fnx"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcb-nirp8p4q"
      },
      "source": [
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "model = load_model('/content/drive/My Drive/WordPrediction/modelbot_200.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJI1JXB68vEq",
        "outputId": "777b3e05-a272-4d6e-bfdf-d2108f898d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_top3_text_model(\"hi\"))\n",
        "print (generate_top3_text_model(\"I\"))\n",
        "print (generate_top3_text_model(\"apply\"))\n",
        "print (generate_top3_text_model(\"sick\"))\n",
        "print (generate_top3_text_model(\"lia\"))\n",
        "print (generate_top3_text_model(\"let's\"))\n",
        "print (generate_top3_text_model(\"cancel\"))\n",
        "print (generate_top3_text_model(\"proceed\"))\n",
        "print (generate_top3_text_model(\"tell\"))\n",
        "print (generate_top3_text_model(\"show\"))\n",
        "print (generate_top3_text_model(\"delete\"))\n",
        "print (generate_top3_text_model(\"quit\"))\n",
        "print (generate_top3_text_model(\"facing\"))\n",
        "print (generate_top3_text_model(\"shift\"))\n",
        "print (generate_top3_text_model(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'there', 'folks', 'friend']\n",
            "['I', 'am', 'want', \"don't\"]\n",
            "['apply', '3', 'courses', 'you']\n",
            "['sick', 'leave', 'employee', 'leaves']\n",
            "['lia', 'what', 'tell', 'how']\n",
            "[\"let's\", 'your', 'the', 'a']\n",
            "['cancel', 'my', 'all', 'it']\n",
            "['proceed', 'now', 'it', 'everything']\n",
            "['tell', 'me', 'do', 'popular']\n",
            "['show', 'me', 'a', 'how']\n",
            "['delete', 'my', 'it', 'me']\n",
            "['quit', 'now', 'the', 'it']\n",
            "['facing', 'new', 'ilt', 'popular']\n",
            "['shift', 'ilt', 'competencies', 'new']\n",
            "['offer', 'the', 'me', 'courses']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8JaLsKT-9zD"
      },
      "source": [
        "def generate_next_texts(seed_text, next_words=5, max_sequence_len=19):\n",
        "\tfor _ in range(next_words):\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\t\t\n",
        "\t\toutput_word = \"\"\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == predicted:\n",
        "\t\t\t\toutput_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\treturn seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlFR0K7dAL4S",
        "outputId": "b70f5ed9-b006-4c20-a3e8-172cb32326f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_next_texts(\"hi\"))\n",
        "print (generate_next_texts(\"I\"))\n",
        "print (generate_next_texts(\"apply\"))\n",
        "print (generate_next_texts(\"sick\"))\n",
        "print (generate_next_texts(\"lia\"))\n",
        "print (generate_next_texts(\"let's\"))\n",
        "print (generate_next_texts(\"cancel\"))\n",
        "print (generate_next_texts(\"proceed\"))\n",
        "print (generate_next_texts(\"tell\"))\n",
        "print (generate_next_texts(\"show\"))\n",
        "print (generate_next_texts(\"delete\"))\n",
        "print (generate_next_texts(\"quit\"))\n",
        "print (generate_next_texts(\"facing\"))\n",
        "print (generate_next_texts(\"shift\"))\n",
        "print (generate_next_texts(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi there it's me my job\n",
            "I am not aware of course\n",
            "apply 3 days leave for me\n",
            "sick leave request the sick leave\n",
            "lia what do you mean by\n",
            "let's your home town you you\n",
            "cancel my flexi leave request my\n",
            "proceed now it that you were\n",
            "tell me what you know about\n",
            "show me list of vilt courses\n",
            "delete my leave request my doubt\n",
            "quit now cancel it cancel it\n",
            "facing new courses means lia what\n",
            "shift ilt courses means now i\n",
            "offer the courses now i guess\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVfOKu3FAfxA"
      },
      "source": [
        "### Retrained on Glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp-jmktSA1wO",
        "outputId": "37939acc-107c-4ca0-bfd8-42c943db75f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, 100,weights=[embedding_matrix], input_length=max_sequence_len-1,trainable = True))\n",
        "model1.add(LSTM(150, return_sequences = True))\n",
        "\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "model1.fit(predictors, label, epochs=200, verbose=1, callbacks=[earlystop])\n",
        "print (model1.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "6166/6166 [==============================] - 12s 2ms/step - loss: 5.3285 - acc: 0.0855\n",
            "Epoch 2/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 5.0629 - acc: 0.0886\n",
            "Epoch 3/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.8134 - acc: 0.1213\n",
            "Epoch 4/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.4539 - acc: 0.1687\n",
            "Epoch 5/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 4.1337 - acc: 0.2454\n",
            "Epoch 6/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.8342 - acc: 0.2882\n",
            "Epoch 7/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 3.5848 - acc: 0.3153\n",
            "Epoch 8/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.3732 - acc: 0.3401\n",
            "Epoch 9/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.2014 - acc: 0.3532\n",
            "Epoch 10/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 3.0487 - acc: 0.3714\n",
            "Epoch 11/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 2.9181 - acc: 0.3837\n",
            "Epoch 12/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.7980 - acc: 0.4025\n",
            "Epoch 13/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.6907 - acc: 0.4152\n",
            "Epoch 14/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.5989 - acc: 0.4257\n",
            "Epoch 15/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.5073 - acc: 0.4380\n",
            "Epoch 16/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.4318 - acc: 0.4444\n",
            "Epoch 17/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.3626 - acc: 0.4473\n",
            "Epoch 18/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.2868 - acc: 0.4632\n",
            "Epoch 19/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.2273 - acc: 0.4661\n",
            "Epoch 20/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.1637 - acc: 0.4737\n",
            "Epoch 21/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.1038 - acc: 0.4869\n",
            "Epoch 22/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 2.0492 - acc: 0.4912\n",
            "Epoch 23/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.9952 - acc: 0.4958\n",
            "Epoch 24/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.9522 - acc: 0.5075\n",
            "Epoch 25/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8988 - acc: 0.5128\n",
            "Epoch 26/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8569 - acc: 0.5209\n",
            "Epoch 27/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.8126 - acc: 0.5282\n",
            "Epoch 28/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.7711 - acc: 0.5360\n",
            "Epoch 29/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.7292 - acc: 0.5485\n",
            "Epoch 30/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.6964 - acc: 0.5488\n",
            "Epoch 31/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.6618 - acc: 0.5582\n",
            "Epoch 32/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.6247 - acc: 0.5584\n",
            "Epoch 33/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5962 - acc: 0.5715\n",
            "Epoch 34/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5585 - acc: 0.5765\n",
            "Epoch 35/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.5327 - acc: 0.5793\n",
            "Epoch 36/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.5105 - acc: 0.5861\n",
            "Epoch 37/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4847 - acc: 0.5931\n",
            "Epoch 38/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4597 - acc: 0.5958\n",
            "Epoch 39/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4355 - acc: 0.5986\n",
            "Epoch 40/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.4140 - acc: 0.6036\n",
            "Epoch 41/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.3920 - acc: 0.6062\n",
            "Epoch 42/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.3703 - acc: 0.6134\n",
            "Epoch 43/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.3495 - acc: 0.6127\n",
            "Epoch 44/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.3378 - acc: 0.6197\n",
            "Epoch 45/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.3187 - acc: 0.6210\n",
            "Epoch 46/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2989 - acc: 0.6252\n",
            "Epoch 47/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2875 - acc: 0.6289\n",
            "Epoch 48/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2725 - acc: 0.6320\n",
            "Epoch 49/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2611 - acc: 0.6252\n",
            "Epoch 50/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2482 - acc: 0.6281\n",
            "Epoch 51/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2353 - acc: 0.6317\n",
            "Epoch 52/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2319 - acc: 0.6343\n",
            "Epoch 53/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.2133 - acc: 0.6398\n",
            "Epoch 54/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1995 - acc: 0.6392\n",
            "Epoch 55/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1896 - acc: 0.6406\n",
            "Epoch 56/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1788 - acc: 0.6419\n",
            "Epoch 57/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1718 - acc: 0.6416\n",
            "Epoch 58/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1597 - acc: 0.6473\n",
            "Epoch 59/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1523 - acc: 0.6448\n",
            "Epoch 60/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1398 - acc: 0.6520\n",
            "Epoch 61/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1333 - acc: 0.6468\n",
            "Epoch 62/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1254 - acc: 0.6505\n",
            "Epoch 63/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1149 - acc: 0.6539\n",
            "Epoch 64/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.1112 - acc: 0.6520\n",
            "Epoch 65/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 1.1043 - acc: 0.6466\n",
            "Epoch 66/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0948 - acc: 0.6560\n",
            "Epoch 67/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0852 - acc: 0.6604\n",
            "Epoch 68/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0836 - acc: 0.6533\n",
            "Epoch 69/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0762 - acc: 0.6572\n",
            "Epoch 70/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0726 - acc: 0.6572\n",
            "Epoch 71/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0687 - acc: 0.6557\n",
            "Epoch 72/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0592 - acc: 0.6584\n",
            "Epoch 73/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0519 - acc: 0.6584\n",
            "Epoch 74/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0498 - acc: 0.6604\n",
            "Epoch 75/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0452 - acc: 0.6565\n",
            "Epoch 76/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0381 - acc: 0.6610\n",
            "Epoch 77/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0332 - acc: 0.6654\n",
            "Epoch 78/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0301 - acc: 0.6622\n",
            "Epoch 79/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0276 - acc: 0.6633\n",
            "Epoch 80/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0237 - acc: 0.6653\n",
            "Epoch 81/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0199 - acc: 0.6649\n",
            "Epoch 82/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0173 - acc: 0.6653\n",
            "Epoch 83/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0133 - acc: 0.6648\n",
            "Epoch 84/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0078 - acc: 0.6682\n",
            "Epoch 85/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0116 - acc: 0.6659\n",
            "Epoch 86/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0050 - acc: 0.6661\n",
            "Epoch 87/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 1.0002 - acc: 0.6680\n",
            "Epoch 88/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9986 - acc: 0.6619\n",
            "Epoch 89/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9922 - acc: 0.6662\n",
            "Epoch 90/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9904 - acc: 0.6666\n",
            "Epoch 91/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9871 - acc: 0.6692\n",
            "Epoch 92/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9855 - acc: 0.6667\n",
            "Epoch 93/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9823 - acc: 0.6666\n",
            "Epoch 94/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9785 - acc: 0.6698\n",
            "Epoch 95/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9815 - acc: 0.6690\n",
            "Epoch 96/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9764 - acc: 0.6690\n",
            "Epoch 97/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9724 - acc: 0.6709\n",
            "Epoch 98/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9743 - acc: 0.6680\n",
            "Epoch 99/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9697 - acc: 0.6721\n",
            "Epoch 100/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9653 - acc: 0.6680\n",
            "Epoch 101/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9680 - acc: 0.6735\n",
            "Epoch 102/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9639 - acc: 0.6693\n",
            "Epoch 103/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9632 - acc: 0.6739\n",
            "Epoch 104/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9606 - acc: 0.6696\n",
            "Epoch 105/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9642 - acc: 0.6653\n",
            "Epoch 106/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9549 - acc: 0.6711\n",
            "Epoch 107/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9611 - acc: 0.6696\n",
            "Epoch 108/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9532 - acc: 0.6695\n",
            "Epoch 109/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9534 - acc: 0.6687\n",
            "Epoch 110/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9541 - acc: 0.6688\n",
            "Epoch 111/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9501 - acc: 0.6732\n",
            "Epoch 112/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9490 - acc: 0.6700\n",
            "Epoch 113/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9523 - acc: 0.6709\n",
            "Epoch 114/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9484 - acc: 0.6692\n",
            "Epoch 115/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9449 - acc: 0.6711\n",
            "Epoch 116/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9445 - acc: 0.6664\n",
            "Epoch 117/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9419 - acc: 0.6721\n",
            "Epoch 118/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9415 - acc: 0.6706\n",
            "Epoch 119/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9406 - acc: 0.6748\n",
            "Epoch 120/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9380 - acc: 0.6705\n",
            "Epoch 121/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9392 - acc: 0.6696\n",
            "Epoch 122/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9352 - acc: 0.6743\n",
            "Epoch 123/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9385 - acc: 0.6669\n",
            "Epoch 124/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9351 - acc: 0.6719\n",
            "Epoch 125/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9356 - acc: 0.6732\n",
            "Epoch 126/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9351 - acc: 0.6726\n",
            "Epoch 127/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9319 - acc: 0.6711\n",
            "Epoch 128/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9298 - acc: 0.6732\n",
            "Epoch 129/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9314 - acc: 0.6726\n",
            "Epoch 130/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9305 - acc: 0.6709\n",
            "Epoch 131/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9314 - acc: 0.6685\n",
            "Epoch 132/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9304 - acc: 0.6708\n",
            "Epoch 133/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9288 - acc: 0.6727\n",
            "Epoch 134/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9305 - acc: 0.6714\n",
            "Epoch 135/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9271 - acc: 0.6773\n",
            "Epoch 136/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9288 - acc: 0.6708\n",
            "Epoch 137/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9263 - acc: 0.6679\n",
            "Epoch 138/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9250 - acc: 0.6760\n",
            "Epoch 139/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9294 - acc: 0.6674\n",
            "Epoch 140/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9211 - acc: 0.6740\n",
            "Epoch 141/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9240 - acc: 0.6713\n",
            "Epoch 142/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9233 - acc: 0.6735\n",
            "Epoch 143/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9247 - acc: 0.6693\n",
            "Epoch 144/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9206 - acc: 0.6734\n",
            "Epoch 145/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9233 - acc: 0.6734\n",
            "Epoch 146/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9210 - acc: 0.6721\n",
            "Epoch 147/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9188 - acc: 0.6722\n",
            "Epoch 148/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9199 - acc: 0.6679\n",
            "Epoch 149/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9188 - acc: 0.6734\n",
            "Epoch 150/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9204 - acc: 0.6672\n",
            "Epoch 151/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9147 - acc: 0.6766\n",
            "Epoch 152/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9165 - acc: 0.6734\n",
            "Epoch 153/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9182 - acc: 0.6732\n",
            "Epoch 154/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9158 - acc: 0.6735\n",
            "Epoch 155/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9176 - acc: 0.6732\n",
            "Epoch 156/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9155 - acc: 0.6740\n",
            "Epoch 157/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9179 - acc: 0.6685\n",
            "Epoch 158/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9156 - acc: 0.6726\n",
            "Epoch 159/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9133 - acc: 0.6695\n",
            "Epoch 160/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9110 - acc: 0.6709\n",
            "Epoch 161/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9128 - acc: 0.6799\n",
            "Epoch 162/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9120 - acc: 0.6709\n",
            "Epoch 163/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9115 - acc: 0.6711\n",
            "Epoch 164/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9127 - acc: 0.6682\n",
            "Epoch 165/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9089 - acc: 0.6753\n",
            "Epoch 166/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9114 - acc: 0.6672\n",
            "Epoch 167/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9087 - acc: 0.6742\n",
            "Epoch 168/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9084 - acc: 0.6734\n",
            "Epoch 169/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9080 - acc: 0.6719\n",
            "Epoch 170/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9084 - acc: 0.6730\n",
            "Epoch 171/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9097 - acc: 0.6706\n",
            "Epoch 172/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9068 - acc: 0.6727\n",
            "Epoch 173/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9072 - acc: 0.6729\n",
            "Epoch 174/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9064 - acc: 0.6765\n",
            "Epoch 175/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9060 - acc: 0.6739\n",
            "Epoch 176/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9098 - acc: 0.6679\n",
            "Epoch 177/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9062 - acc: 0.6730\n",
            "Epoch 178/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9078 - acc: 0.6706\n",
            "Epoch 179/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9085 - acc: 0.6747\n",
            "Epoch 180/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9068 - acc: 0.6753\n",
            "Epoch 181/200\n",
            "6166/6166 [==============================] - 11s 2ms/step - loss: 0.9033 - acc: 0.6745\n",
            "Epoch 182/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9048 - acc: 0.6690\n",
            "Epoch 183/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9040 - acc: 0.6765\n",
            "Epoch 184/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9031 - acc: 0.6779\n",
            "Epoch 185/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9054 - acc: 0.6729\n",
            "Epoch 186/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9053 - acc: 0.6703\n",
            "Epoch 187/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9074 - acc: 0.6760\n",
            "Epoch 188/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9028 - acc: 0.6730\n",
            "Epoch 189/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9038 - acc: 0.6735\n",
            "Epoch 190/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9042 - acc: 0.6752\n",
            "Epoch 191/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9034 - acc: 0.6732\n",
            "Epoch 192/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9021 - acc: 0.6732\n",
            "Epoch 193/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9028 - acc: 0.6724\n",
            "Epoch 194/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9009 - acc: 0.6708\n",
            "Epoch 195/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9022 - acc: 0.6714\n",
            "Epoch 196/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9009 - acc: 0.6708\n",
            "Epoch 197/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9041 - acc: 0.6730\n",
            "Epoch 198/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9034 - acc: 0.6696\n",
            "Epoch 199/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8995 - acc: 0.6685\n",
            "Epoch 200/200\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.9009 - acc: 0.6729\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 18, 100)           71800     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 18, 150)           150600    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               100400    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 718)               72518     \n",
            "=================================================================\n",
            "Total params: 395,318\n",
            "Trainable params: 395,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3_xNILnA8-h"
      },
      "source": [
        "model1.save('/content/drive/My Drive/WordPrediction/modelbotRetrained_200.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "del model1  # deletes the existing model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6jO1qHtJEJr",
        "outputId": "680df890-8610-43ec-e2da-fca23620d085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "model1 = load_model('/content/drive/My Drive/WordPrediction/modelbotRetrained_200.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOG_UD55YEFn"
      },
      "source": [
        "def generate_top3_text_model_retrainable(seed_text, max_sequence_len=19):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model1.predict(token_list, verbose=0)\n",
        "    flattened_predict = predicted.flatten()\n",
        "    pred_idx_list = flattened_predict.argsort()[:-4:-1]\n",
        "    output_word = [seed_text]\n",
        "    for i in pred_idx_list:\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == i:\n",
        "                output_word.append(word)\n",
        "                break\n",
        "    return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPFodBbnYiK-",
        "outputId": "84544a34-1e71-4032-b88e-e75777f98a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_top3_text_model_retrainable(\"hi\"))\n",
        "print (generate_top3_text_model_retrainable(\"I\"))\n",
        "print (generate_top3_text_model_retrainable(\"apply\"))\n",
        "print (generate_top3_text_model_retrainable(\"sick\"))\n",
        "print (generate_top3_text_model_retrainable(\"lia\"))\n",
        "print (generate_top3_text_model_retrainable(\"let's\"))\n",
        "print (generate_top3_text_model_retrainable(\"cancel\"))\n",
        "print (generate_top3_text_model_retrainable(\"proceed\"))\n",
        "print (generate_top3_text_model_retrainable(\"tell\"))\n",
        "print (generate_top3_text_model_retrainable(\"show\"))\n",
        "print (generate_top3_text_model_retrainable(\"delete\"))\n",
        "print (generate_top3_text_model_retrainable(\"quit\"))\n",
        "print (generate_top3_text_model_retrainable(\"facing\"))\n",
        "print (generate_top3_text_model_retrainable(\"shift\"))\n",
        "print (generate_top3_text_model_retrainable(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'there', 'hi', 'mister']\n",
            "['I', 'am', 'want', \"don't\"]\n",
            "['apply', '3', 'robot', 'for']\n",
            "['sick', 'leave', 'leaves', 'sick']\n",
            "['lia', 'what', 'tell', 'how']\n",
            "[\"let's\", 'proceed', 'it', 'what']\n",
            "['cancel', 'my', 'all', 'it']\n",
            "['proceed', 'now', 'a', 'it']\n",
            "['tell', 'me', 'the', 'about']\n",
            "['show', 'me', 'more', 'do']\n",
            "['delete', 'my', 'sessions', 'it']\n",
            "['quit', 'the', 'i', 'that']\n",
            "['facing', 'enroll', 'about', 'is']\n",
            "['shift', 'lot', 'good', 'great']\n",
            "['offer', 'me', 'the', 'any']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oteui6HhY2Dv"
      },
      "source": [
        "def generate_next_texts_retrainable(seed_text, next_words=3, max_sequence_len=19):\n",
        "\tfor _ in range(next_words):\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t\tpredicted = model1.predict_classes(token_list, verbose=0)\n",
        "\t\t\n",
        "\t\toutput_word = \"\"\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == predicted:\n",
        "\t\t\t\toutput_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\treturn seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rqq4Gs0ZYOK",
        "outputId": "7216456b-13ab-441e-b630-42f7757e220a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_next_texts_retrainable(\"hi\"))\n",
        "print (generate_next_texts_retrainable(\"I\"))\n",
        "print (generate_next_texts_retrainable(\"apply\"))\n",
        "print (generate_next_texts_retrainable(\"sick\"))\n",
        "print (generate_next_texts_retrainable(\"lia\"))\n",
        "print (generate_next_texts_retrainable(\"let's\"))\n",
        "print (generate_next_texts_retrainable(\"cancel\"))\n",
        "print (generate_next_texts_retrainable(\"proceed\"))\n",
        "print (generate_next_texts_retrainable(\"tell\"))\n",
        "print (generate_next_texts_retrainable(\"show\"))\n",
        "print (generate_next_texts_retrainable(\"delete\"))\n",
        "print (generate_next_texts_retrainable(\"quit\"))\n",
        "print (generate_next_texts_retrainable(\"facing\"))\n",
        "print (generate_next_texts_retrainable(\"shift\"))\n",
        "print (generate_next_texts_retrainable(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi there it's me later it\n",
            "I am not aware of course\n",
            "apply 3 days leave for me\n",
            "sick leave at it a wasteland\n",
            "lia what do you mean by\n",
            "let's proceed it how you were\n",
            "cancel my leave request me some\n",
            "proceed now bot please tell me\n",
            "tell me what you know about\n",
            "show me list of vilt courses\n",
            "delete my leave request me your\n",
            "quit the bot it is for\n",
            "facing enroll means you were put\n",
            "shift lot talking to you later\n",
            "offer me any courses for me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4rkN1pxbElb"
      },
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXXCyIZZZsVy",
        "outputId": "a8ba3dcc-3efe-4ef6-dc38-77bf0aed3dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Continue training\n",
        "model1.fit(predictors, label, epochs=300, verbose=1, callbacks=[earlystop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8970 - acc: 0.6726\n",
            "Epoch 2/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8921 - acc: 0.6735\n",
            "Epoch 3/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8932 - acc: 0.6687\n",
            "Epoch 4/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8921 - acc: 0.6737\n",
            "Epoch 5/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8901 - acc: 0.6756\n",
            "Epoch 6/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8927 - acc: 0.6721\n",
            "Epoch 7/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8943 - acc: 0.6717\n",
            "Epoch 8/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8937 - acc: 0.6756\n",
            "Epoch 9/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8951 - acc: 0.6735\n",
            "Epoch 10/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8912 - acc: 0.6747\n",
            "Epoch 11/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8915 - acc: 0.6726\n",
            "Epoch 12/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8909 - acc: 0.6727\n",
            "Epoch 13/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8894 - acc: 0.6734\n",
            "Epoch 14/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8915 - acc: 0.6735\n",
            "Epoch 15/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8913 - acc: 0.6724\n",
            "Epoch 16/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8941 - acc: 0.6752\n",
            "Epoch 17/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8917 - acc: 0.6760\n",
            "Epoch 18/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8911 - acc: 0.6760\n",
            "Epoch 19/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8922 - acc: 0.6734\n",
            "Epoch 20/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8895 - acc: 0.6774\n",
            "Epoch 21/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8915 - acc: 0.6743\n",
            "Epoch 22/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8917 - acc: 0.6729\n",
            "Epoch 23/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8958 - acc: 0.6758\n",
            "Epoch 24/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8907 - acc: 0.6740\n",
            "Epoch 25/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8902 - acc: 0.6726\n",
            "Epoch 26/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8944 - acc: 0.6784\n",
            "Epoch 27/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8943 - acc: 0.6743\n",
            "Epoch 28/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8917 - acc: 0.6730\n",
            "Epoch 29/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8904 - acc: 0.6742\n",
            "Epoch 30/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8878 - acc: 0.6745\n",
            "Epoch 31/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8877 - acc: 0.6769\n",
            "Epoch 32/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8898 - acc: 0.6739\n",
            "Epoch 33/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8897 - acc: 0.6726\n",
            "Epoch 34/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8882 - acc: 0.6747\n",
            "Epoch 35/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8873 - acc: 0.6739\n",
            "Epoch 36/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8880 - acc: 0.6776\n",
            "Epoch 37/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8876 - acc: 0.6782\n",
            "Epoch 38/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8910 - acc: 0.6745\n",
            "Epoch 39/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8908 - acc: 0.6737\n",
            "Epoch 40/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8881 - acc: 0.6760\n",
            "Epoch 41/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8893 - acc: 0.6729\n",
            "Epoch 42/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8881 - acc: 0.6729\n",
            "Epoch 43/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8881 - acc: 0.6719\n",
            "Epoch 44/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8894 - acc: 0.6737\n",
            "Epoch 45/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8886 - acc: 0.6700\n",
            "Epoch 46/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8943 - acc: 0.6743\n",
            "Epoch 47/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8893 - acc: 0.6737\n",
            "Epoch 48/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8867 - acc: 0.6734\n",
            "Epoch 49/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8872 - acc: 0.6703\n",
            "Epoch 50/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8873 - acc: 0.6789\n",
            "Epoch 51/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8930 - acc: 0.6679\n",
            "Epoch 52/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8896 - acc: 0.6747\n",
            "Epoch 53/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8892 - acc: 0.6756\n",
            "Epoch 54/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8970 - acc: 0.6713\n",
            "Epoch 55/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8872 - acc: 0.6753\n",
            "Epoch 56/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8886 - acc: 0.6737\n",
            "Epoch 57/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8904 - acc: 0.6795\n",
            "Epoch 58/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8874 - acc: 0.6724\n",
            "Epoch 59/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8856 - acc: 0.6742\n",
            "Epoch 60/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8851 - acc: 0.6729\n",
            "Epoch 61/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8882 - acc: 0.6753\n",
            "Epoch 62/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8916 - acc: 0.6753\n",
            "Epoch 63/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8873 - acc: 0.6739\n",
            "Epoch 64/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8882 - acc: 0.6719\n",
            "Epoch 65/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8868 - acc: 0.6758\n",
            "Epoch 66/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8853 - acc: 0.6774\n",
            "Epoch 67/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8847 - acc: 0.6729\n",
            "Epoch 68/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8852 - acc: 0.6700\n",
            "Epoch 69/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8846 - acc: 0.6747\n",
            "Epoch 70/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8868 - acc: 0.6734\n",
            "Epoch 71/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8857 - acc: 0.6752\n",
            "Epoch 72/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8848 - acc: 0.6734\n",
            "Epoch 73/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8847 - acc: 0.6786\n",
            "Epoch 74/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8874 - acc: 0.6747\n",
            "Epoch 75/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8854 - acc: 0.6737\n",
            "Epoch 76/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8828 - acc: 0.6748\n",
            "Epoch 77/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8883 - acc: 0.6729\n",
            "Epoch 78/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8857 - acc: 0.6752\n",
            "Epoch 79/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8886 - acc: 0.6743\n",
            "Epoch 80/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8876 - acc: 0.6763\n",
            "Epoch 81/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8861 - acc: 0.6748\n",
            "Epoch 82/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8852 - acc: 0.6745\n",
            "Epoch 83/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8853 - acc: 0.6761\n",
            "Epoch 84/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8842 - acc: 0.6750\n",
            "Epoch 85/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8879 - acc: 0.6722\n",
            "Epoch 86/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8850 - acc: 0.6740\n",
            "Epoch 87/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8863 - acc: 0.6789\n",
            "Epoch 88/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8852 - acc: 0.6752\n",
            "Epoch 89/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8859 - acc: 0.6753\n",
            "Epoch 90/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8857 - acc: 0.6748\n",
            "Epoch 91/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8878 - acc: 0.6730\n",
            "Epoch 92/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8858 - acc: 0.6758\n",
            "Epoch 93/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8851 - acc: 0.6766\n",
            "Epoch 94/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8863 - acc: 0.6748\n",
            "Epoch 95/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8828 - acc: 0.6768\n",
            "Epoch 96/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8797 - acc: 0.6774\n",
            "Epoch 97/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8832 - acc: 0.6760\n",
            "Epoch 98/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8851 - acc: 0.6773\n",
            "Epoch 99/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8852 - acc: 0.6747\n",
            "Epoch 100/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8845 - acc: 0.6752\n",
            "Epoch 101/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8835 - acc: 0.6786\n",
            "Epoch 102/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8833 - acc: 0.6765\n",
            "Epoch 103/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8857 - acc: 0.6740\n",
            "Epoch 104/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8856 - acc: 0.6782\n",
            "Epoch 105/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8906 - acc: 0.6742\n",
            "Epoch 106/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8849 - acc: 0.6794\n",
            "Epoch 107/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8846 - acc: 0.6763\n",
            "Epoch 108/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8840 - acc: 0.6782\n",
            "Epoch 109/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8825 - acc: 0.6755\n",
            "Epoch 110/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8832 - acc: 0.6714\n",
            "Epoch 111/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8838 - acc: 0.6716\n",
            "Epoch 112/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8837 - acc: 0.6734\n",
            "Epoch 113/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8813 - acc: 0.6758\n",
            "Epoch 114/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8825 - acc: 0.6774\n",
            "Epoch 115/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8809 - acc: 0.6787\n",
            "Epoch 116/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8838 - acc: 0.6752\n",
            "Epoch 117/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8851 - acc: 0.6755\n",
            "Epoch 118/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8846 - acc: 0.6800\n",
            "Epoch 119/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8816 - acc: 0.6742\n",
            "Epoch 120/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8857 - acc: 0.6766\n",
            "Epoch 121/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8860 - acc: 0.6768\n",
            "Epoch 122/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8855 - acc: 0.6706\n",
            "Epoch 123/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8895 - acc: 0.6695\n",
            "Epoch 124/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8897 - acc: 0.6779\n",
            "Epoch 125/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8914 - acc: 0.6730\n",
            "Epoch 126/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8908 - acc: 0.6755\n",
            "Epoch 127/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8847 - acc: 0.6735\n",
            "Epoch 128/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8926 - acc: 0.6735\n",
            "Epoch 129/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8853 - acc: 0.6721\n",
            "Epoch 130/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8839 - acc: 0.6761\n",
            "Epoch 131/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8845 - acc: 0.6722\n",
            "Epoch 132/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8830 - acc: 0.6763\n",
            "Epoch 133/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8803 - acc: 0.6734\n",
            "Epoch 134/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8804 - acc: 0.6763\n",
            "Epoch 135/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8807 - acc: 0.6743\n",
            "Epoch 136/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8791 - acc: 0.6760\n",
            "Epoch 137/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8809 - acc: 0.6779\n",
            "Epoch 138/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8819 - acc: 0.6716\n",
            "Epoch 139/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8821 - acc: 0.6743\n",
            "Epoch 140/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8807 - acc: 0.6768\n",
            "Epoch 141/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8811 - acc: 0.6745\n",
            "Epoch 142/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8828 - acc: 0.6726\n",
            "Epoch 143/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8890 - acc: 0.6752\n",
            "Epoch 144/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8816 - acc: 0.6800\n",
            "Epoch 145/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8827 - acc: 0.6756\n",
            "Epoch 146/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8859 - acc: 0.6742\n",
            "Epoch 147/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8883 - acc: 0.6771\n",
            "Epoch 148/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8827 - acc: 0.6761\n",
            "Epoch 149/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8814 - acc: 0.6753\n",
            "Epoch 150/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8793 - acc: 0.6800\n",
            "Epoch 151/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8811 - acc: 0.6745\n",
            "Epoch 152/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8771 - acc: 0.6761\n",
            "Epoch 153/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8810 - acc: 0.6737\n",
            "Epoch 154/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8810 - acc: 0.6716\n",
            "Epoch 155/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8850 - acc: 0.6765\n",
            "Epoch 156/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8813 - acc: 0.6752\n",
            "Epoch 157/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8780 - acc: 0.6748\n",
            "Epoch 158/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8786 - acc: 0.6776\n",
            "Epoch 159/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8805 - acc: 0.6739\n",
            "Epoch 160/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8802 - acc: 0.6774\n",
            "Epoch 161/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8793 - acc: 0.6766\n",
            "Epoch 162/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8804 - acc: 0.6714\n",
            "Epoch 163/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8792 - acc: 0.6729\n",
            "Epoch 164/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8790 - acc: 0.6740\n",
            "Epoch 165/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8808 - acc: 0.6755\n",
            "Epoch 166/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8790 - acc: 0.6747\n",
            "Epoch 167/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8787 - acc: 0.6734\n",
            "Epoch 168/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8982 - acc: 0.6717\n",
            "Epoch 169/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8866 - acc: 0.6730\n",
            "Epoch 170/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8798 - acc: 0.6732\n",
            "Epoch 171/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8793 - acc: 0.6742\n",
            "Epoch 172/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8773 - acc: 0.6739\n",
            "Epoch 173/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8789 - acc: 0.6753\n",
            "Epoch 174/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8795 - acc: 0.6730\n",
            "Epoch 175/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8784 - acc: 0.6742\n",
            "Epoch 176/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8781 - acc: 0.6789\n",
            "Epoch 177/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8787 - acc: 0.6787\n",
            "Epoch 178/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8790 - acc: 0.6739\n",
            "Epoch 179/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8805 - acc: 0.6750\n",
            "Epoch 180/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8794 - acc: 0.6737\n",
            "Epoch 181/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8781 - acc: 0.6761\n",
            "Epoch 182/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8797 - acc: 0.6758\n",
            "Epoch 183/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8791 - acc: 0.6769\n",
            "Epoch 184/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8802 - acc: 0.6750\n",
            "Epoch 185/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8801 - acc: 0.6750\n",
            "Epoch 186/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8800 - acc: 0.6732\n",
            "Epoch 187/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8794 - acc: 0.6771\n",
            "Epoch 188/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8814 - acc: 0.6779\n",
            "Epoch 189/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8893 - acc: 0.6747\n",
            "Epoch 190/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8884 - acc: 0.6742\n",
            "Epoch 191/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8835 - acc: 0.6787\n",
            "Epoch 192/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8799 - acc: 0.6752\n",
            "Epoch 193/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8778 - acc: 0.6734\n",
            "Epoch 194/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8776 - acc: 0.6768\n",
            "Epoch 195/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8757 - acc: 0.6776\n",
            "Epoch 196/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8763 - acc: 0.6748\n",
            "Epoch 197/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8780 - acc: 0.6779\n",
            "Epoch 198/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8784 - acc: 0.6753\n",
            "Epoch 199/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8793 - acc: 0.6750\n",
            "Epoch 200/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8767 - acc: 0.6743\n",
            "Epoch 201/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8773 - acc: 0.6760\n",
            "Epoch 202/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8770 - acc: 0.6769\n",
            "Epoch 203/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8770 - acc: 0.6779\n",
            "Epoch 204/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8785 - acc: 0.6743\n",
            "Epoch 205/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8778 - acc: 0.6774\n",
            "Epoch 206/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8806 - acc: 0.6752\n",
            "Epoch 207/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8782 - acc: 0.6763\n",
            "Epoch 208/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8781 - acc: 0.6781\n",
            "Epoch 209/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8914 - acc: 0.6777\n",
            "Epoch 210/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8849 - acc: 0.6760\n",
            "Epoch 211/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8822 - acc: 0.6753\n",
            "Epoch 212/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8785 - acc: 0.6748\n",
            "Epoch 213/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8746 - acc: 0.6755\n",
            "Epoch 214/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8776 - acc: 0.6742\n",
            "Epoch 215/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8777 - acc: 0.6734\n",
            "Epoch 216/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8768 - acc: 0.6737\n",
            "Epoch 217/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8773 - acc: 0.6760\n",
            "Epoch 218/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8757 - acc: 0.6737\n",
            "Epoch 219/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8764 - acc: 0.6755\n",
            "Epoch 220/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8779 - acc: 0.6771\n",
            "Epoch 221/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8780 - acc: 0.6743\n",
            "Epoch 222/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8784 - acc: 0.6727\n",
            "Epoch 223/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8790 - acc: 0.6758\n",
            "Epoch 224/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8777 - acc: 0.6768\n",
            "Epoch 225/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8771 - acc: 0.6799\n",
            "Epoch 226/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8787 - acc: 0.6727\n",
            "Epoch 227/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8808 - acc: 0.6729\n",
            "Epoch 228/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8770 - acc: 0.6761\n",
            "Epoch 229/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8786 - acc: 0.6763\n",
            "Epoch 230/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8785 - acc: 0.6792\n",
            "Epoch 231/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8739 - acc: 0.6779\n",
            "Epoch 232/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8761 - acc: 0.6743\n",
            "Epoch 233/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8797 - acc: 0.6732\n",
            "Epoch 234/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8776 - acc: 0.6732\n",
            "Epoch 235/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8791 - acc: 0.6732\n",
            "Epoch 236/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8763 - acc: 0.6792\n",
            "Epoch 237/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8783 - acc: 0.6779\n",
            "Epoch 238/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8904 - acc: 0.6748\n",
            "Epoch 239/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8849 - acc: 0.6769\n",
            "Epoch 240/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8780 - acc: 0.6739\n",
            "Epoch 241/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8785 - acc: 0.6750\n",
            "Epoch 242/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8779 - acc: 0.6758\n",
            "Epoch 243/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8767 - acc: 0.6750\n",
            "Epoch 244/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8749 - acc: 0.6752\n",
            "Epoch 245/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8737 - acc: 0.6761\n",
            "Epoch 246/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8751 - acc: 0.6777\n",
            "Epoch 247/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8743 - acc: 0.6795\n",
            "Epoch 248/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8735 - acc: 0.6752\n",
            "Epoch 249/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8751 - acc: 0.6753\n",
            "Epoch 250/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8774 - acc: 0.6747\n",
            "Epoch 251/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8749 - acc: 0.6795\n",
            "Epoch 252/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8754 - acc: 0.6807\n",
            "Epoch 253/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8759 - acc: 0.6760\n",
            "Epoch 254/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8797 - acc: 0.6752\n",
            "Epoch 255/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8772 - acc: 0.6737\n",
            "Epoch 256/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8779 - acc: 0.6773\n",
            "Epoch 257/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8747 - acc: 0.6763\n",
            "Epoch 258/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8736 - acc: 0.6769\n",
            "Epoch 259/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8753 - acc: 0.6771\n",
            "Epoch 260/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8785 - acc: 0.6753\n",
            "Epoch 261/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8765 - acc: 0.6771\n",
            "Epoch 262/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8761 - acc: 0.6739\n",
            "Epoch 263/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8777 - acc: 0.6740\n",
            "Epoch 264/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8789 - acc: 0.6773\n",
            "Epoch 265/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8755 - acc: 0.6790\n",
            "Epoch 266/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8762 - acc: 0.6765\n",
            "Epoch 267/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8736 - acc: 0.6790\n",
            "Epoch 268/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8742 - acc: 0.6777\n",
            "Epoch 269/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8736 - acc: 0.6748\n",
            "Epoch 270/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8757 - acc: 0.6756\n",
            "Epoch 271/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8755 - acc: 0.6790\n",
            "Epoch 272/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8780 - acc: 0.6729\n",
            "Epoch 273/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8794 - acc: 0.6795\n",
            "Epoch 274/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8841 - acc: 0.6774\n",
            "Epoch 275/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8781 - acc: 0.6773\n",
            "Epoch 276/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8755 - acc: 0.6742\n",
            "Epoch 277/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8750 - acc: 0.6763\n",
            "Epoch 278/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8737 - acc: 0.6774\n",
            "Epoch 279/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8732 - acc: 0.6779\n",
            "Epoch 280/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8733 - acc: 0.6768\n",
            "Epoch 281/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8761 - acc: 0.6717\n",
            "Epoch 282/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8741 - acc: 0.6774\n",
            "Epoch 283/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8745 - acc: 0.6763\n",
            "Epoch 284/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8744 - acc: 0.6742\n",
            "Epoch 285/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8751 - acc: 0.6769\n",
            "Epoch 286/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8740 - acc: 0.6758\n",
            "Epoch 287/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8747 - acc: 0.6769\n",
            "Epoch 288/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8738 - acc: 0.6779\n",
            "Epoch 289/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8751 - acc: 0.6750\n",
            "Epoch 290/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8753 - acc: 0.6784\n",
            "Epoch 291/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8767 - acc: 0.6763\n",
            "Epoch 292/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8750 - acc: 0.6792\n",
            "Epoch 293/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8779 - acc: 0.6765\n",
            "Epoch 294/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8831 - acc: 0.6726\n",
            "Epoch 295/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8782 - acc: 0.6766\n",
            "Epoch 296/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8754 - acc: 0.6777\n",
            "Epoch 297/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8758 - acc: 0.6765\n",
            "Epoch 298/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8740 - acc: 0.6756\n",
            "Epoch 299/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8726 - acc: 0.6786\n",
            "Epoch 300/300\n",
            "6166/6166 [==============================] - 10s 2ms/step - loss: 0.8744 - acc: 0.6781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f9db73048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7NlEqikaobk"
      },
      "source": [
        "model1.save('/content/drive/My Drive/WordPrediction/modelbotRetrained_500.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "del model1  # deletes the existing model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZAxnLypR4P"
      },
      "source": [
        "model1 = load_model('/content/drive/My Drive/WordPrediction/modelbotRetrained_500.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBhoSfXLpZLQ",
        "outputId": "f36b2d97-d377-4c5a-9094-134fb3017daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_top3_text_model_retrainable(\"hi\"))\n",
        "print (generate_top3_text_model_retrainable(\"I\"))\n",
        "print (generate_top3_text_model_retrainable(\"apply\"))\n",
        "print (generate_top3_text_model_retrainable(\"sick\"))\n",
        "print (generate_top3_text_model_retrainable(\"lia\"))\n",
        "print (generate_top3_text_model_retrainable(\"let's\"))\n",
        "print (generate_top3_text_model_retrainable(\"cancel\"))\n",
        "print (generate_top3_text_model_retrainable(\"proceed\"))\n",
        "print (generate_top3_text_model_retrainable(\"tell\"))\n",
        "print (generate_top3_text_model_retrainable(\"show\"))\n",
        "print (generate_top3_text_model_retrainable(\"delete\"))\n",
        "print (generate_top3_text_model_retrainable(\"quit\"))\n",
        "print (generate_top3_text_model_retrainable(\"facing\"))\n",
        "print (generate_top3_text_model_retrainable(\"shift\"))\n",
        "print (generate_top3_text_model_retrainable(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'there', 'folks', 'bot']\n",
            "['I', 'am', 'want', \"don't\"]\n",
            "['apply', '3', 'robot', 'meaning']\n",
            "['sick', 'leave', 'sessions', 'leaves']\n",
            "['lia', 'what', 'tell', 'how']\n",
            "[\"let's\", 'proceed', 'it', 'talk']\n",
            "['cancel', 'my', 'all', 'it']\n",
            "['proceed', 'now', 'a', 'i']\n",
            "['tell', 'me', 'the', 'about']\n",
            "['show', 'me', 'how', 'a']\n",
            "['delete', 'my', 'the', 'it']\n",
            "['quit', 'the', 'you', 'popular']\n",
            "['facing', 'enroll', 'course', 'about']\n",
            "['shift', 'cream', 'life', 'modes']\n",
            "['offer', 'the', 'me', 'courses']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLDXFrvppDi",
        "outputId": "6ee61acf-fd50-41d8-db30-cdb45c26164e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print (generate_next_texts_retrainable(\"hi\"))\n",
        "print (generate_next_texts_retrainable(\"I\"))\n",
        "print (generate_next_texts_retrainable(\"apply\"))\n",
        "print (generate_next_texts_retrainable(\"sick\"))\n",
        "print (generate_next_texts_retrainable(\"lia\"))\n",
        "print (generate_next_texts_retrainable(\"let's\"))\n",
        "print (generate_next_texts_retrainable(\"cancel\"))\n",
        "print (generate_next_texts_retrainable(\"proceed\"))\n",
        "print (generate_next_texts_retrainable(\"tell\"))\n",
        "print (generate_next_texts_retrainable(\"show\"))\n",
        "print (generate_next_texts_retrainable(\"delete\"))\n",
        "print (generate_next_texts_retrainable(\"quit\"))\n",
        "print (generate_next_texts_retrainable(\"facing\"))\n",
        "print (generate_next_texts_retrainable(\"shift\"))\n",
        "print (generate_next_texts_retrainable(\"offer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi there it's me\n",
            "I am not aware\n",
            "apply 3 days leave\n",
            "sick leave request it\n",
            "lia what do you\n",
            "let's proceed it wrong\n",
            "cancel my request request\n",
            "proceed now cancel my\n",
            "tell me what you\n",
            "show me list of\n",
            "delete my leave request\n",
            "quit the bot the\n",
            "facing enroll means you\n",
            "shift cream but a\n",
            "offer the courses now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmKKaIUop3fA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}